Trancript of https://www.youtube.com/watch?v=8XCXvzQdcug&t=5382s by Chris Christodoulou 
This transcript was ripped from the transcript box in youtube, dumped into excel as sentences, used Excel autofilter to remove timecodes/noise by hand, then broken into quarters to fit into the automatic punctuator at http://bark.phon.ioc.ee/punctuator

Text follows:

My hope with this content today is that we can give a bit of an overview of some of the catastrophic risks that the world currently faces. Why the solutions to some of those risks can actually make other of the risks worse. Why the solutions to the risk landscape as a whole can create dystopias, because to have the power to be able to check all the catastrophic risks then uh becomes a a very constant can become a very concentrated power that is itself hard to check. All of these risks are unsolvable without solving the things the patterns of human behavior that give rise to them, but that the patterns of human behavior that give rise to them can be named and can be progressively not perfectly but progressively better addressed, and that in doing So there is a possibility to create a attractor for the world that is not catastrophes or dystopias, in which we have the responsibility and wisdom adequate individually and collectively to steward the power of the exponential technology that we are creating and the hope of this. As a introduction is that people might feel that it helps them make sense of what feels like lots of different problems in the world in a way that is hopefully clarifying and integrating, and they can have more people engage in the kind of innovation and collective intelligence Towards solutions to the deepest underlying dynamics, yeah, so in the conversations that you and i had leading up to deciding to do this piece, i realized that we have done these pieces on this channel before on sensemaking, which many people engaged with and responded to, and i Was really happy to do together and that the entire topic of sense making is a facet of this larger topic of what we're going to explore today? What you know, i might kind of call and shorthand the metacrisis metacrisis, meaning climate change and many different environmental issues from topsoil erosion to dead zones in ocean to pollinators to um biodiversity loss. You know species extinction etc to a lot of new existential risks. As a result of new emerging fields of exponential technology, ai biotech nanotech drone cyber etc to the intersection of a economic system that requires continuous exponential growth with a planet that is reaching planetary boundaries that all of these are uh very serious issues and thinking about any Of them can seem kind of overwhelming thinking about all of them together. The only way i have found uh to think about how we navigate that meaningfully is to start to explore. Do all of these issues have anything in common. Are there any dynamics about how human coordination and collective action and patterns of human choice, making and patterns of human choice, making intersecting with things like markets and states and technology work that both give rise to these kind of human-induced problems and make them challenging to shift And both that collective set of crises and the underlying dynamics that give rise to them is what we think of as kind of the the metacrisis, which is, i would say, a very a meaningful way to orient to what humanity needs to address. At this current phase and the issue with sense making is a part of that. If we can't agree on what the nature of the issues are, that need addressed or what good solutions look like, then some percentage of the people go working on a thing they think is important, like a particular approach to climate change and the other half of the Population that radically disagrees with them does everything they can to prevent that from happening or since we can't all agree to do a particular thing like nuclear disarmament or not doing the ai weapons or making the cost of carbon higher or to deal with externalities. If we can't all internationally agree to that, anyone that consciously chooses to do that disadvantages themselves in a geopolitical game of power relative to others. Therefore nobody can do the effective thing, and then you just have kind of collective action failures that create races to the bottom. If we can't have some better shared sense, making of what needs done to inform better coordination, collective choice making, obviously we don't even have like a starting point. So all the conversations on sense making are one really critical part of this and i'm not going to readdress those, because we've done that on the channel. People can check it out if they're interested, i'm going to try to just do like apply some sense, making to look at some frameworks for how to make sense of what's going on that might lead to design criteria for what more adequate solutions could look like that. Hopefully, get more people engaging in in innovative generative ways along these lines, the exponential power of exponential tech makes via decentralization catastrophes more likely as and it doesn't matter which one, whether it's this ai scenario, or that ai scenario, this bioscenario, this environmental scenario or this warfare Scenario, we just put them all in the bucket called cascading catastrophes or to be able to bind those catastrophes. It has to bind all of them, because any of the catastrophes are enough to break the thing down to bind. All of them requires really centralized power or one possibility is really centralized power. I'M going to say that there's another possibility if it goes decentralized power capacity, then that ends up looking like dystopias. Those are the two attractors that civilization is currently most likely heading in and we can see how the solutions to either one orient to the other one more. If you try to say we don't want these dystopic things, so we want to stop authoritarian tech. We want to decentralize and democratize technological capabilities. Then we get more collective action, failures and decentralized, catastrophic tech. If you're like, we don't want this, we want some surveillance on this and whatever then we get more dystopic tech, so i would say that we are seeking a third attractor that is neither catastrophes or dystopias seems like a pretty fair starting place, for how do we Design, a civilization that is desirable, or at least allows us to continue to advance in the on the meaningful questions. What a desirable civilization is um involves being able to avoid the twin attractors of catastrophes and dystopias, and that those two must always be taken in consideration. Because it's very easy for a partial solution to externalize a problem somewhere else, as we've mentioned before that one of the major problems with the world is the problem with our problem. Solving we define the problem generally externalize harm somewhere else that these we can solve for one catastrophic risk while making another one more likely. We can also solve for catastrophic risk as a whole, making dystopias more likely. So we want to be able to hold the undesirable possibilities that are in relationship to each other. At the same time, we want to be able to hold all the problems at the same time, to then be able to say what does a solution look like. It decreases the likelihood for all of those simultaneously since such a third attractor is defined, at least to start as being something that avoids the other two attractors of catastrophes or dystopias. We'Re going to start by sketching out what the catastrophic risk landscape and the generative dynamics that give rise to it. Look like and what the kind of dystopic solution dynamics could look like, so that we understand the underlying dynamics that have to be solved as a necessary. But not sufficient starting point. I want to share a few things in preface before diving in first. Is that the concepts that i'm sharing are not my concepts? I claim no propriety. I'Ve been fortunate to be in dialogue for many years, a lot of uh, really important thinkers in the field of existential risk in the environment and economics, and you know also study a lot of people's work. So i'm hoping to share some of the frameworks that i find most useful. That is mostly a synthesis i wish i could do good attribution in real time for all of it, but in a podcast form where it's going to be edited, it's tricky. So what i'd say is i have a blog civilizationemerging.com? There'S a resources section. You can look at the books, articles um collaborators there and you'll see where a lot of you know the provenance of the ideas came from. You can also go to the consilience project and the articles there are all well cited and you can also see uh. You know some of the team and advisors people i've worked with, so that's just one important part next important part in preface is. I absolutely don't claim that the frameworks i'm sharing are complete or adequate to make sense of everything. Um, i'm continuously uh refining, my own understanding of it in this process. So i this is halfway through 2022. That'S my best current understanding of things. I'Ve, no doubt that that will improve, and that's actually one of the reasons to share this is so that if people have thoughts on better frameworks or errors that are here or additions, that more collective intelligence can be working in this direction of what are the most Fundamental things to think about and work on, for advancing what is most critical, that's not already being tended to. I also want to share that when we think about global catastrophic risks and global existential risks uh in particular, it can be a very overwhelming and disheartening topic and um there's just a reality to that. My hope is that i would neither leave anyone in a disheartened place, nor do i want to kind of present the bypass that there isn't uh a reality to feeling you know whatever feelings you feel about um the real challenges in the state of the world um For what it's worth, i've been focused on these issues for a very long time i felt existential angst and sadness and despair and fear and all kinds of things about it. I continue to work on it because i also feel hope and a real sense of possibility to navigate through these things and so for whatever it's worth. If you have not been if you're newer to this study uh as someone who has been looking at accidental risk for a very long time across lots of sectors, i will just share for what it's worth. I don't feel um that this is a hopeless situation. I wouldn't be sharing here. If i did, i feel like the solutions that are needed. Very much are threading the eye of a needle, but i both have a sense of possible solutions, possible architectures of a world that can be stable, metastable in the presence of exponential technologies and pathways to enact those solutions. It'S not a given that we will, but it's also not a given that we won't. It feels still very much like a thing that our collective choices are determining the outcome of, and so the hope in sharing. This is uh that more people's intelligence and care makes a difference in uh navigating these things. I also want to say that there are a lot of really important things that need done other than trying to figure out meta-systemic stuff about how to navigate the metacrisis, and so, if there's work, that you're doing on that is aligned with this. Whether it's work in animal rights or environmental work or social justice work or you're, making intentional communities to prototype some things or community gardens, and that feels like what is yours to do and a meaningful application of your agency. I don't want focusing on the scale and complexity of these issues to be disheartening or make it seem like what you're doing is less meaningful. All those things have to keep happening and are really important for the some people who are oriented to try to work on. How do we change our macroeconomic systems, our governance systems, our collective sense, making and choice, making systems the way we think about problem? Solving writ large the way we think about tech or at large what we think about education. The way we deal with regulation of exponential tech. Hopefully this is useful there, and even for people who are not necessarily going to be working in that area, but just having a better s. Sense of the things you're already sensing and intuiting in some way is at least helpful for orientation. That would be my hope. I think for the people who are probably drawn to listen to this, i might share frameworks and details that you're not familiar with, but it it probably resonates with an intuition that many people have um. I i think i mean i can share a little bit about just like brief aspects of my journey of like what got me into thinking about this um, because i think it's actually. I think the things that got me thinking about are pretty common. I think i had some uncommon exposure to things that allowed me to you know, find different frameworks and put more time into the topics but um. I i i was exposed to environmental work and animal rights, work and extreme poverty work and things when i was pretty young. My parents were into the topics and just getting engaged in kind of the field of activism. You see a particular thing like the dolphins in the cove or factory farms, or what's happening in the amazon or the congo or whatever, and that one thing seems worthy of the dedication of a whole life. It seems so um catastrophic and unacceptable and um, and then, when you start to see lots of those things, uh there's some sense that i had that. I feel that i've heard many people have of like they're. They seem somehow interconnected. They seem somehow like expressions of some underlying patterns of how humanity is operating, that uh collectively are reaching a point, as we have eight billion people, globalization, high technology, high resource use per capita that they can't continue. This way, they're not just a bummer anymore, they're, um, they're, somehow kind of self-terminating, um and all somehow interconnected. I think that i think some sense that there is a fundamental kind of reorientation that is not just a continuation of the dialectic of progress, but is a kind of epochal phase shift of some kind. It was the first conversation you and i had it's about this type of epochal phase shift. I think an intuition like that is not uncommon um, so that you know that kind of uh was there from pretty early in my exploration of the topics and uh, something that was maybe a little bit unique in my experience was. I was engaged in these areas of activism, but also because of being homeschooled and because of what my parents were into. I was reading buck, mr fuller and kind of design, science considerations, reading fritz off capra and the kind of ecological systems thinkers that talked about the inner connectivity of it uh reading david baume and people who were and krishnamurti who were talking about the fragmentation of our Worldview being underlying to all of the problems, the ability to identify with an in-group and go into rivalry with an out-group, the ability to optimize a particular metric while externalizing harm to other metrics, that all the ability to benefit the near term while harming the long term. That all of those were special case. They were individual instances of the general thing of perceiving parts rather than holes where we would try to optimize parts in ways that damage the holes, which ended up actually damaging the parts. Those kinds of world views i was exposed to early, and so i was thinking about what are the underlying patterns of all these problems. What could solutions to those things look like, and specifically i got to see that some of the really well intended activism that did actually succeed in solving particular problems ended up unintentionally, causing other problems in the process, and i think there'd been a lot of critiques of Philanthropy and people have come to understand this - that i've spoke about this in other places, but the the first one where this really hit me and then - and then i saw this everywhere - was a project that was working on stopping elephant poaching in a particular uh preserve And when you see how sentient elephants are, when you see how kind of gruesome the poaching is for making rings or whatever out of ivory um doing whatever you can to just stop, it is a very reasonable response, but the solution was bigger. Fences around the uh preserve to keep poachers out and legislation for harsher sentencing for poachers for elephants, particularly and after a huge amount of work to have those things occur. The poaching of the white rhino and the mountain gorilla increased from some of the same people, because the underlying poverty of the people hadn't changed a macroeconomic system that creates poverty at scale. Hadn'T changed the value systems towards animals. Hadn'T changed an economic system where animals alive are not worth anything on anyone's balance sheet, but animals dead are the like. They were all those underlying drivers, hadn't shifted and the other animals that were being poached now were actually more endangered, and so not only did it not shift the underlying causes, it arguably made other it made the the whole global situation worse and that was really devastating To me, because, like the first devastation was seeing uh seeing these issues at all, it was uh factory farms. For me, it was the first one, and so then the solution was there is a thing called activism and we can make the things better. The next devastation was seeing that the way we would try to make it better often made it worse and not in an obvious way, because it did make it better for those elephants there right, but as we zoom out. If our goal is not these elephants here, but is the kind of thriving of all life and perpetuity as best as possible, uh then the way we define the problem has to get deeper, and so we see that um gdp as a metric of the success of A nation is not an adequate metric uh, it does seem to like, in so far as you believe, market ideology, that gdp going up means more products and services that improve people's quality of life goes up. It seems like a good idea insofar as you realize that war makes gdp go up and sick people engaging in more health expenses does and addiction makes it go up. You realize. Okay and the gdp corresponds to environmental and sustainability. Gdp is not an adequate metric in the same way that that's the case. We can start to over focus on the metric of these elephants here or carbon reductionism. The whole focus is get co2 out of the air, but we can do that through mechanisms that cause other environmental or social problems. There'S a need to kind of step back and say our approach to problem solving is actually one of the problems. One of the underlying kind of generators is because we define the problems in too narrow a way, and so when we define a problem in a narrow way, then we can create a solution to that narrow problem. That does benefit that thing, but that interacts with a lot of other things and ends up causing externalities or harm in those other areas. And so when we think about defining problems in a narrow way in a world that is actually interconnected and we're separating out from that interconnectedness, a particular part that we care about want to advance, we can - and this is kind of what is unique to humans. Our capacity to understand particular parts with abstraction in a way that allows us to build tools, our our technology creation capacities are one of the obvious things that is unique to uh. To sapiens also allows us to advance parts, irrespective of and in ways that are harmful to holes, that end up creating these other issues so um. So one of the defining kind of features of the metacrisis that became clear early on for me was that our way of thinking about problems is one of the problems and that we actually have to have a much more holistic way of thinking about uh. What is worth advancing, what are all of the things that are driving a particular problem and what are the possible things that will come out of a particular solution that might create other problems to other metrics to other areas, to other agents you know to other beings And how to factor all of that into what solutions are actually solutions worth advancing? It was long tangent on saying. I think that the intuition that we're at some kind of inflection point that there are is something about the nature of how we think about things and how we go about solving problems and how we go about advancing things. That there is some change that has to occur at a deep level like that. I think this is not an uncommon intuition. People will kind of come to things like uh greed and um things about motivating patterns of human behavior, seeming, like the obvious places to start uh. The kind of real politic assessment that um humans are uh too irrational to make the good choices and or too rivalrous, to make the good choices and that we're in problems kind of as a result of us being very good at making very powerful tech. And not very good at uh long range and more inclusive considerations. I think these are like, i said somewhat common intuitions. What i, what i hope to do, is to deepen some of these and clarify them in ways that might actually start to highlight uh. What adequate solutions that are actually inactible could look like in terms of a high-tech civilization that are actually responsible. Stewards of the power of that tech all right so as we dive into assessing where civilization currently is uh, i want to talk about the actual, catastrophic risks that we face, give some examples of them uh and some timelines and um. What is actually novel about this? Historically and what the support tends for the future, and if we want a different feature than the path we're currently on what it might take to do that um, i want to give a preface first that when we try to do any kind of like long arc Of history, um big picture, orienting frameworks uh, we have to be careful with it there are it it's easy to make those over simplifying and then become a source of um kind of confirmation bias. So there are, there are a couple types of uh historical narrative that often occur here that i think are both uh problematic. One is kind of the hobzyan dialectic of progress perspective that um pre-agricultural early people were had lives that were brutish mean short and nasty, and it's been kind of a upward ascent from the mud since then, and that technology science and capitalism have been the candle in The darkness, illuminating and making everything better and that we simply need to keep doing more of those, and it will continue to get better and better and there's certainly plenty of popular books and popular thinkers that explicate this narrative and there's certainly truth to it. Nobody wants to go to dentistry and a time before, novocaine on and on that perspective generally leaves out the rat, the unbelievable amount of species extinction and environmental harm and increased movement towards uh catastrophic risk. Writ large uh leaves out ubiquitous mental health issues that were probably not evolutionarily prevalent in the same way, and you know a lot lots of other things like that and has a cartoonishly negative view of past people and also a homogenizingly cartoonishly negative view. The other one is the view that kind of over romanticizes the living in perfect harmony and balance with nature indigenous people and that it's been kind of a fall from the garden or fall from redemption type model, and that we have to get back to that early Wisdom and um, i don't think that the way that people lived hundreds of thousands of years ago gives, in relative with relatively much lower tech, much lower populations. Much smaller population sizes has adequate solutions for what we do with a population of this size and global supply chains and exponential technologies. That doesn't mean it has nothing to share. I think there is actually a lot that can be shared, but we're not going to find adequate solutions there. I think first, it's just valuable to see that if you look around the planet today and take samplings of people from very different parts of the world or different subcultures, they're really different, right and uh. Similarly, if you go back into the past before the plow, the idea that they were all kind of brutish nasty and mean or all kind of enlightened is pretty silly, there's, probably a pretty wide distribution of the values and the sustainability and the enlightenment. If you would call it of the people - and we see that early civilizations rose to certain kinds of heights and then collapsed, and so we also see that there was neither a kind of continuous descent nor continuous ascent or something that had cycles in it. Where how much knowledge was lost in the burning of alexandria, how much was lost in the fall of sumeria or other places? When we see things like the anti-catheria mechanism, which is like a metal geared clock, that is thousands of years old? That was not supposed to be the case or, like oh, the the historical narrative of just kind of um progress, not probably not very true um. So it's important whenever we try to do these kind of sweeping assessments that we understand that at any given time there was a pretty wide distribution of the different types of civilizations and through time there would be fluctuations within those all that said uh. I think some patterns that we can see pretty clearly is that from all of recorded history, that we have any good evidence for the total level of technology that we have possessed since the industrial revolution and progressively throughout that time, in particular, since the digital revolution is Historically, unprecedented, the total population size and the interconnectivity of that globally through global supply chains is also totally unprecedented. I think that's that's pretty straightforward. That'S not that hard to argue, so i want to talk about the state of catastrophic risk facing the world today and kind of a historical perspective on that. It'S fair to say that human induced existential risk, because we always face the possibility of a meteor or a solar flare or something outside of you know like some kind of natural phenomena that could wipe us out, but that's different than us doing something that wipes ourselves Out right, so human-induced catastrophic risk can be said to have started in world war ii with the nuclear bomb that, before that, we just didn't have enough technological power to destroy that much that quickly. Even if we wanted to that as effective as he was genghis. Khan. Still couldn't make the planet not habitable, and so one important insight when we think about catastrophic risk is human-induced catastrophic risk. Is it is inexorably tied to the level of technological development, because the technology is the amongst? Other things is a lever for how much power our choices have. It also ends up influencing the type of choices we make, which is an important point we'll get to later, but to just start with. For now, it obviously impacts the scope of our choices and people with stone tools. Just can't destroy the biosphere and just can't create a world war that destroys everybody, people with bronze tools or iron tools. Also can't people with nukes can, and so the nuclear bomb was the first time - and you know people recognized this at the time is like um. When you see a mushroom cloud - and you then hear the stories of zeus's lightning bolt or whatever the mushroom cloud was a bigger destructive force than most of the kind of mythos of what the power of gods in different cultures had been. So it's like. Oh, we have something like the power of god, so we can split atoms and destroy things at an unbelievable scale, and then we can go into something like mutual assure destruction and do that at a global scale. Do we have something like the wisdom of gods and the love and the prudence to be able to steward and guide the power of it adequately? That'S one of the kind of deep orienting questions and frameworks, uh that i come back to, and so it's fair to say that the bomb was the beginning of the level of technology where human-induced catastrophic risk was actually a thing. It'S important to say that before that catastrophic risk at the level of individual civilizations was a real thing, and not only was it a real thing, it was it pretty much happened to every civilization. If you uh read books like the uh collapse of complex societies by tainter that are doing analyses of the fall of previous empires, one of the things in and you just kind of think of the history of it. One of the things that most empires have in common is that they don't still exist that they either went through gradual decline or um, not gradual, quick failures, which can happen. Sometimes they happen militarily. Sometimes they happen through their own environmental and sustainability. Topsoil depletion using up all the trees things like that. So i think it's an important point that if people are not thinking about this generally, they might not be in the front of their awareness that human civilizations have had lifespans and they've ended, and they have largely ended from self-induced causes that either they were, they would Grow in a way that was unsustainable to the environment and collapse for those reasons, or they would uh, engage in resource conflicts to be able to have continuous growth that engendered more enmity with others that eventually led to their war. Many of them fell from rivals that were less capable than previous rivals that they had defended against because they went through a internal erosion. They became so powerful that then they started infighting and, having you know things like polarization and lack of coordination. Where, then, even smaller rivals could fight them, so there was something like institutional or civilizational decay that occurred as a result of their success oftentimes. So it's important to get that the early civilizations went extinct and they went extinct largely for self-induced causes that a enduring human civilization is a thing that we don't have a model for so far that what is novel now is that we have a truly global civilization. For the first time that as big as the aztec or inca or egyptian empires, or even all the way up to something like the roman empire, it was not truly a fully global system. But, given that the computer or tv that you're, probably watching this on required six continent supply chains to be able to build, meaning that the the fundamental technologies that mediate, the way that we live today require globalized supply chains that no countries can make the technology by Themselves that they need, we really do. It is fair to say we don't have a chinese civilization and a u.s civilization, since they are not autonomous right. We really do have a global civilization that, i would say, is also in the process of self-induced collapse and that but the scope of that collapse, rather than just kind of local environmental destruction. Something like destruction of the habitability of the biosphere at large or rather than a local war. Something like you know, totalizing war is really unprecedented um, but of course those those civilizations did face the experience of existential threat to themselves um. So it is important to understand we're not talking about like some unprecedented kind of uh, excessively negative thing, we're talking about something that has happened a bunch of times just never in a fully global context. So then we get to world war ii and we say okay, so the bomb gave us the ability to uh have global scale catastrophic risk for the first time, and because of that it we had to not do that. It became very clear we had to actually for the first time in human history. We had a technology so powerfully destructive that we had to ensure we never used it, and before that, every time we had a technology, there was a race to deploy it as quickly as possible for everybody to be able to use it for advantage. This is the first time we had a technology that, rather than race, to use it, we had to ensure that nobody ever used it now. Of course, there was still a race to maximize the total amount of stockpile and things like that, and so an entire world system was created after world war ii to prevent the use of the bomb again, and i would say that that post-world war ii system was Successful at preventing nuclear exchange in world war iii of that kind, kinetic world war three up till now, i would say that it also advanced. It was responsible as a solution to a problem that was too narrowly defined. That caused other problems. That world system was also responsible for advancing many of the catastrophic risks we now face and that that whole world system is ending and i'll speak to the structures of this briefly. So the the world system we created after world war ii was one mutually sure destruction. How do we make sure that these two superpowers are not going to fire the weapon? Let'S make sure that uh nobody feels like they. Nobody believes they could win it, that it would be lose lose for everybody, so that nobody in does that thing. That was one part of the solution. Another part was the new global monetary system, the bretton woods monetary system and the um kind of reserve currency system and like that, that allowed for exponential growth of the uh global economy and the idea there being that um resources. Economics were one of the main drivers of war historically, because everybody wanted more stuff and that if there isn't a larger pie than to get more stuff, you have to take each other's stuff, which causes war so that the way to prevent war is a continuously growing Pie, so everybody can have more stuff without taking each other's stuff. Of course, where does that stuff comes from that meant unrenewable depletion and pollution of the environment, which brings us to all the planetary boundaries that we're you know about to hit so exponential growth of the monetary system? Year-Over-Year exponential growth means uh exponential, uh, unrenewable, depletion of resources and exponential pollution, both of which have happened. You don't get to do that forever on a finite planet, so running an exponential financial system on a linear materials economy that is taking unrenewable resources and turning it into unprocessable. Waste brings us to all these different planetary tipping points, and we can see depletion of topsoil and trace minerals, depletion of trees, depletion of fish. You know depletion of species depletion of pollinators on this side and we can see them the accumulation of co2 and mining waste and you know microplastics in the ocean and so many other things here uh. But we can see that all those environmental problems are the side effects. The second order unintended consequences of the solution to the. How do we not have nuclear war issue, which is how do we grow the economy? Um? Another major aspect of the post-world war ii solution was globalization and having these radically interconnected global supply chains, so that, since we depended upon each other to make stuff, we had lessons sent up to bomb each other because we'd be bombing our own supply chains and so That global interdependence, um there's a lot of people in the environmental movement focused on wanting you know: local production move everything back to kind of local. The movement to globalization was actually really important, which was it made us invested in each other's success rather than bombing each other. But it also produced a world where collapses of of supply chains anywhere can start to lead to kind of cascading catastrophic shocks everywhere, and we saw that really clearly with cobit. We saw that a problem in one part of the world in muhan started to have cascading issues, not just the movement of the virus, but to be able to stop the movement of the virus. Stopping travel, which also meant shutting down supply chains, which meant that fertilizers and pesticides didn't get where they needed to which meant uh. You know devastated crops from locusts in northern africa and parts of the middle east and crop failure in india and which created uh, more food shortage of for hundreds of millions of people, and you know, risks there than probably um the total risk for attended by covet Itself, um: we got to start to see how a world this interconnected supply chain wise can have local issues really lead to global catastrophic issues. So these were some of the main aspects of the post-world war ii solution and i would say that while they served a purpose, they brought us to the point that now we have reached planetary boundaries across lots of different axes where both the unrenewability or running out Of unrenewable resource and running into the un unsustainability of the pollution issue means we cannot continue to exponentially, grow the gdp on a linear materials economy. What do we do? Instead? That doesn't then also cause war from shrinking gdp and having people continue to want more stuff. Like this is where we have to hold the various tensions together and not say then: okay, now the problem is the environmental one. So let's go ahead and try to solve that by lowering use in gdp which by itself is a very nearly impossible thing to do, because if you don't get everyone to agree to it, whoever voluntarily does it geopolitically disadvantages themselves so much that nobody's gon na. Do it but um also, if then, anyone is or you know, if anyone is uh oriented towards continuing to grow their population resource et cetera, then, and in a shrinking pie situation, then war increases. So we have to think about the environmental issues, the economic issues, the geopolitical issues all together to be able to have solutions to any of them. The next part of the kind of post-world war ii system that is no longer viable is the idea of mutually sure destruction on technology that we shouldn't use. Measure destruction is a way of kind of forcing an equilibrium forcing a nash equilibrium which, when you have one catastrophe weapon, and you have two players that have it and it's one that is easy to monitor, because there are not that many uranium mines and enriching uranium Takes huge cyclotrons that you can see from space and whatever you can do mutual destruction in that way, when you start to get to a world where um, because of ai weapons and bioweapons and cyber weapons on infrastructure, and like that, you have many different types of Catastrophe, weapons and many different players that have them, not all of whom are nation states. You cannot put a f a mutation or destruction or force nash equilibrium on that in the same way, so we have advanced to a world where more states have new nuclear weapons. More other states have that don't have nuclear weapons, have the ability to do disinformation, attacks and things that could lead to nuclear escalation between the countries that do have them and the other kinds of catastrophic weapons. Nuclear weapons are very hard to build right. The the actual hardware involved is very hard to build. This is not true with ai swarm intelligence, empowered drone weapons that can take out massive infrastructure targets that can be built by small groups of people. This is not true with bio weapons that, particularly with gene drives and tabletop crispr, and these technologies advancing right now many areas of biotech, synthetic bio and genetic engineering are advancing faster than moore's law, which you know we hear all these things in the kind of positive Silicon valley, world about the democratization of technological power, is a good thing. The democratization of catastrophic power, not necessarily a good thing right, like the the idea that um power that can be used intentionally or accidentally right, say someone's, not building biotechnology intentionally as bioweapons. But biosecurity is a hard thing technology that can be catastrophic, being radically decentralized, that is not monitorable and where you have a very hard time being able to enforce agreements. Uh creates radical fragility in the world, so um small actors, not nation-state actors with no special exotic materials in the next few years. As these various categories of artificial intelligence and cyber capacities and drone capacities and biotech capacities get cheaper and cheaper, more and more powerful um. How does the world make it through decentralized, catastrophic capability, right catastrophe, weapons for anybody who wants them capability and it's important to get even that again? Nukes are really hard to build, so you have to keep the knowledge of nukes secret, but even if the knowledge was out they're hard to build when we're talking about something like artificial intelligence or cyber technologies, where you don't really have to build any hardware and the Software is not that hard to build and you can build it on cloud computing infrastructures that are kind of equally that are easily open and available to everybody. Then even scientific publishing, portends catastrophic risk because, let's say uh, a major research institution is advancing knowledge of how to do uh, swarming drones or whatever. It is um a breakthrough in a model for ai. They have ethical review boards for the purposes that they're doing it. For but they figure out a capacity, they publish it. Whoever now just reads that doesn't have an ethical review board on what they do with it, and so we can it. We can advance a technology for a positive purpose and have it unintentionally cause harms in other ways, just through the result of using it. For that positive purpose, we can also advance the technology for a positive purpose, and the underlying knowledge gets repurposed for all other kinds of purposes, and so i would say that we are in a situation again that is so radically unprecedented, not just in the world before World war ii, but in the whole world, since world war ii, until really just now and over the next handful of years, where even open access to knowledge and information in the fields of advancing technology, poor tends catastrophic risk potential that is unmonitorable. And so we we have to take that seriously and say what we would do about that um. So, for these reasons that you can't do mutually assured destruction in the current landscape, you can't keep an exponential growth economy with linear supply chains that we don't have one catastrophic risk. We have many that are in tensions with each other, where the things that you do to decrease the likelihood of war might increase environmental risk. The things that you do to benefit a particular environmental risk might increase the likelihood of war. I would say the post-world war ii world, as a you know, the pax americana world is fundamentally over is inadequate to the landscape of the problems we face. The problems we face are historically novel are near-term catastrophic, not on one axis, but many many different axes and are without adequate solutions currently, and so that's kind of a an introductory frame for what i would call the metacrisis. The current state of affairs that we have to understand clearly to be able to take responsibility to address, because, obviously we want to address this. So i want to give just a little bit more detail of what the relatively near term, what a few of the relatively near-term catastrophes could look like across a few different sectors to have a sense of what we have to address right. These create boundary conditions or kind of forcing functions for things that we need to address. This will be by no means comprehensive in terms of the risk landscape and there's a lot of good work that has been published on this a lot of places if people are not familiar with nick bostrom's work and places like the future of humanity institute, i would Say check it out, reading his vulnerable worlds, hypothesis papers, a good start, so climate change, there's, obviously pretty radical debate on um how much temperature change, how much sea level rise? How quickly, how much extreme weather events are attributable um, but i think that there are increasing extreme weather. Events is clear that uh extreme droughts and heat waves have been increasing over the last few years, and everything indicates that they will continue to increase and that we don't need to get to the place of like runaway venusification of the planet. To start to look at climate change leading to catastrophic risk, if we look at right now, as we're heading into the summer, the heat waves that are happening in india and if we look at the last couple, summers where they've been like almost 50 celsius, heat waves In a in some areas of high population, high population density that co-occur with running out of groundwater and not having access to air conditioning and other things would be necessary. When we look at shocks to food supply storage that happened during covid and heat waves, it could get hot enough to destroy crops and the amount of people. Then in food insecurity do we see a situation where the next few summers could have heat waves that displace large numbers of people much larger than we have dealt with anytime? Recently, tens of millions or more people becoming climate change refugees? This is really not unlikely and when we look at what's happened after the previous refugee crisis, um, syria and other ones there's no place that is really interested in taking tens of millions of refugees and people don't just die peacefully in those situations. So let's say we look at a situation like northern india that has both huge population density, huge population, a lot of resource and security and a place where the heat waves can hit under resource shortage. Do you get resource wars? Do those resource wars, possibly cleave along politically tense lines that already exist, maybe muslim, hindu lines or things like that? Can those escalate to things like india, pakistan, wars which are both nuclear-equipped countries? This is an example, and there are many other ones. That'S just one example of the way that uh climate change can lead to resource shortages. It can lead to human migration that can lead to wars that could lead to escalation on existing geopolitical tension lines that hit technological amplifiers like places that have nukes or kind of weapons of mass destruction. And so you see this kind of cascade effect where, before climate change is a catastrophic risk by itself, just through environmental phenomena, it can be the beginning of a cascade. I think there are quite a few things like that in the next single digit number of years that have a much higher probability that are under mitigated. So that's one set of things to look at um and i think that since covid and since the radical kind of polarization in the u.s between the left and right, that has expressed itself through the kind of january 6 activity at the capitol the um. The huge polarization around trump and social media, the um george floyd uh, triggered social justice issues in the u.s as well. As you know, a pretty decent percentage of the continent of australia catching on fire and then barely even staying in the news. Because follow the things and then, of course, the war with ukraine and tensions over taiwan. I think there is a sense of um the instability of the world situation that is much more widespread than it was five years ago um and not just the instability, but the complexity of it and the fact that there is not one catastrophic risk. There'S a lot of things happening kind of simultaneously and that's trying that the probability of each of those plus the total number of scenarios and the number of escalation pathways is increasing each year rather than decreasing. So if we just kept trying to deal with what are the three nearest catastrophic risks and how do we mitigate those, we buy very little time so thinking about okay? How what do we have to do to change like i said all of these are the result of tech, and this is either the result of the cumulative results of industrial tech. So all the environmental issues, the planetary boundary issues of which climate change is one - are pretty much. Industrial tech multiplied by globalization, having cumulative effects over some hundreds of years and now reaching tipping points. And then nuclear and other advanced military tech adds to that, and then exponential tech adds to that right. So the catastrophic risk landscape is kind of cumulative industrial plus, particularly exponential the categories of exponential tech and what we mean by exponential i'll, try to there's a few different ways. We could define it. We could say any technology that improves itself recursively nukes, don't make better nukes, but computers do make better computers. I can use computers to model how to make better computer chips, so you get moore's law type dynamics, so technology that recursively has the capacity for supporting its own self-improvement, creates an exponential curve technologies that allow exponentially larger impact per dollar or number of people or unit Time so we see facebook and google getting to three billion users in like a decade compared to uh the scaling at which u.s standard oil or whatever got to nowhere near that many people over a much longer period of time. We can see that a much smaller number of people can have a much larger impact, much faster, so exponential speed of effect, exponential scale of effect, which also means more players able to have a bad level of effect. When we look at the myriad of environmental issues that we face, that we said are a result of the cumulative effects of industrial tech with increasing population and increasing gdp demands. It'S illustrative to be able to have that kind of overarching framework to look at it. As opposed to just individual areas, because if we look at say climate change as one environmental risk, that uh is largely a result of excessive co2 from the burning of carbon-based things, mostly fossil fuels. Obviously there are methane and agriculture and other things, but let's just take take the carbon part. So as far as the supply chain goes, that's kind of the waste or the output side of using hydrocarbons as fuel. The other side is that we're getting diminishing returns on hydrocarbons, that we have a society that depends upon hydrocarbons as the energy source and, while we're figuring out creative ways like tar sands and shale and fracking and offshore drilling, to find the rest of them. We'Re having to go to harder more difficult areas, because we've already got most of the easy ones and it takes a certain amount of oil to get more oil. So the energy return on energy investment. How much oil it takes to get a new barrel of oil is increasing when you look at the fact that the total amount of gdp is very closely coupled with the total amount of energy use globally and the gdp has to go up exponentially just to keep Up with interest, that means that there is a exponential demand for the total amount of energy used, but we're getting a diminishing return on hydrocarbons and the new non-hydrocarbon sources of energy have required more hydrocarbons to make and they have a certain energy return on energy. Investment of how long it takes you know for them to pay off the there's, a there's, a massive kind of gap, um and uh, and reckoning on that side. So obviously you have all of the like. You said: mountaintop removal, mining for coals and the environmental harm of that, and oil spills and geopolitical destabilization over oil on the side of getting it, as well as the underlying deep issues between the inner global energy system and the global finance system. That is also just the other side of the supply chain of climate change, so you can see climate change when you look at the entirety of the energy system. You'Re, like wow, there's some other, really really critical and timely issues that are just part of looking at the movement of hydrocarbons to the system, let alone everything else so um. If you want to think about all of the environmental issues. From the point of view of nature, having mostly closed-loop processes that anything, any type of waste in nature is just food for something else in a time scale that uh doesn't lead to either waste accumulation or depletion of a needed resource. Every dead thing and every type of byproduct, feces or whatever is fully metabolized by everything else: ecosystem that that is not the same for human technology and relationship with the ecosystem that we utilize materials that much faster than the earth can regenerate them. Or we can regenerate them through other sources and turn them into wastes much faster than nature can turn them back into anything else, and we have real environmental harms from both of those. So the idea of a linear materials economy that is undergoing exponential growth on a finite planet. That thing has to shift, so we both have to deal with the embedded growth obligation and finance for exponential growth, and we have to deal with the closed loop on our materials economy so that we are making all of our new stuff from old stuff and all Of our old stuff is not waste or pollution, but it's getting you know cycled back in to make new stuff and that we're doing that with the energy bandwidth actually available to us. That'S an easy thing to say what it actually takes to do, that factoring global coordination and the incentives not to do it and the fact that so many of the actual real costs are externalized and that when you internalize them, almost no industry would actually be profitable And markets wouldn't be viable, there's some deep, there's some deep issues there, but that's um yeah a framework for looking at the what has to happen underneath changing the environmental issues kind of very large when we think about the exponential tech mediated issues. One framework for thinking about that is that there is a perverse incentive, a kind of um perverse incentive if people are not already familiar with the concept is another concept that you can see pretty quickly underlying most of the problems in the world, not just the catastrophic Risks just the suck that we have dealt with in lots of industries, perverse incentive means where any individual or corporation or nation, state or agent actor of any kind has some type of incentive. That is misaligned with the well-being of other actors or the comments, and so they are incented to do something that is directly causing harm or externalizing harm somewhere else. It'S pretty easy to see that a for-profit military-industrial complex that would go out of business if there was no war or there were adequate solutions to peace or for-profit health care structure. The way it is, or whatever we can see like perverse incentives are not a hard thing to see. So that would be an example of something that is kind of underneath and driving at the level of the nature of economic structure and how we regulate that and to really find adequate solutions. I think actually requires changes at the level of how we create currency issues. Regarding the fungibility of currency and property ownership, private property, deep, deep topics, but if we um, if we were to think about what are some underlying drivers of the things that have been problems for forever. But at this level of technological capacity and nearing planetary tipping points. Where those problems actually become catastrophes, perverse incentive would be one of those drivers, which then also means that it creates a basis for an orienting question, which is the orienting question would be for humanity to think about. How do we identify and close perverse incentive gaps? Procedurally everywhere, there's not a perfect answer to that, because whatever answer you have, then there will be a way to gain that answer, but the ability to or the orientation to progressively seek better and better answers and implement them towards that um, i think, is deeper than Trying to solve climate change or trying to do healthcare, reform or any other particular topic like that. Similarly, the question: how do we identify externalities and internalize them like force them into the cost equation and into the responsibility equation? There is no perfect answer to that, but seeking better and better ways to be able to do that having more collective intelligence focused on that is upstream from any particular problem, we can identify so there's a particular kind of perverse incentive, um associated with the development of New technologies in particular, which is that there is much more incentive to focus on opportunities than of a new technology than risks of a new technology so and a new technology will obviously have both right. The technology is um, it's only going to proliferate if it does provide some advantage. Um. So, of course, there's going to be opportunity, that's associated uh, but because it's going to interact with a complex world, there's going to be risks that are associated with it. If i focus, if i don't want to advance a technology that might harm the world meaningfully, so i am going to do really deep risk assessment. I'M going to think through what are all of the ways that this could cause risk, both through its direct physical externalities. As well as the ways it changes social dynamics because whoever uses it gets more power in some ways, so what types of people will be, how will it change the power landscape and how it will change the nature of psyches, and things like that? If i really try to do good thoughtful assessment of where the second third fourth order risks associated are and either not build the technology if they are too bad, and i can't mitigate them or figure out how to design the technology differently to mitigate those or make Sure that the technology also launches with particular regulation in place or whatever, then i'm probably not going to be the first to market, i might not get to market at all. I might just decide not to do the thing if, on the other hand, i just focus on the positive application and probably whatever the nearest term most profitable, positive application is, and i either don't do risk assessment at all, or i do a pro forma version. That is mostly about box checking to say that i did risk assessment while really not wanting anything that slows the movement uh then i'll, probably get first mover advantage. I'Ll probably get the ability to scale, and especially in network dynamics, be the one who gets to benefit from metcalfe's law and kind of network monopolies and, as a result, there is a kind of race to the bottom of anyone who would want who's aware that there Are risks either doesn't matter because they won't end up getting the power or they end up having to say well, i can't focus on those now. Otherwise i won't be a player and i won't be able to do anything so i have to. I have to pretend that the risks are going to be solvable later and still rush to get ahead. So at least then i have the the capital in the market position to hopefully be able to implement solutions later, which of course, is pretty much impossible, because then there will be a fiduciary responsibility to continue to maximize shareholder advantage. There will be inertia already in place and things like that. This is a really tricky issue right. It'S a really tricky issue because uh the incentive to move fast and break things, externalize and socialize the losses but privatize the gains, and that even if i feel like i want to be an ethical actor, i still want to be able to do stuff. And so i want to gain as much power as i can, and if i don't do the thing it still doesn't prevent the thing, because other people are going to do it anyways how we make it through gaining exponential influence capacities across domains as fundamental as the Genome and the base code of life and the you know: artificial intelligence getting increasingly generalized, and things like that like this is a very deep fundamental thing to deal with, and so i there's a great video i'll see if we can link it in the comments to This that somebody put out on leaded gasoline and the development of the lead additive that made a motor stop, knocking and the actual health risks that were known even to the scientists at the time that they lied about and head that led to leaded gasoline proliferating globally. That led to huge amounts of lead toxicity in the biosphere not just affecting human life, but also human life, and that a little bit of the effects known of that was that the lead decreases. Human iq significantly and the collective effect of that across the whole population is something like billions of points of iq loss globally and that it makes people more violent. I think it was something like a forex increase in violence being exposed to it again across the entire populations like global populations. We come back to think about how a lot of people like to write off that we can't make a better civilization, because humans are too dumb and too nasty, and we see just lead, makes us dumber and nastier and we're like well how dumb and nasty are We is not a unchangeable metric, it's actually changeable by a lot of things. Are there some cultures that have developed a much higher level of base education across their whole culture? Yes, you know, like jews, have done a better job of education than a lot of other cultures. Have are there some that have done a better job at non-violence? Yes, jains and buddhists and quakers are pretty nonviolent across large populations. Some populations are radically violent across the whole population, so there's and that's not even getting into things like lead. So now you think about you think about this one atom for the purpose of stopping engine knocking they got through where, by the time we saw all the harm associated and regulated it out. We had already lost billions of iq points. You don't get rid of the lead in the environment anytime, quickly, right, that's going to take for forever, and so the fact of regulation happening after the fact, because the market moved the thing forward. It hid what it knew about the problems and or was never incentivized to figure it out to begin with it lobbied in its own interest, and so only once the problem was so gruesome that it had so much support. Could regulation happen. After the fact, we see that with ddt, we see that with cigarettes. We see that with you know so many things. Do we get to do that with exponential tech, where, by the time the problem is so gruesome we regulate it, nope like whether we're talking about ai or biotech it is too late. At that point, the consequences are too significant, too fast-moving and too self-catalytic. So this portends a radically different approach to the relationship between the market, technology and regulation, and you know whatever culture ends up in forming regulation. We have the idea of a liberal democracy where the the law, the right the basis of regulation, is to be the result of the collective values of the people through democratic process. So the the culture uh is supposed to be advancing human education and development and informedness. So that people have progressively more developed value systems that then they vote to have coded into law. Where then, the monopoly of violence that the state has can regulate the market where the market would have reverse incentives so with biotech risk, there's a lot of different things. We'Re talking about we're talking about things associated with synthetic biology which is uh advancing faster than moore's law and things associated with crispr and different kind of gene. Editing technologies, both as a result of things that could be used for positive applications like agriculture or waste management or whatever, but that could produce unintentional consequences as a result of putting you know, genetically modified organisms designed to benefit one thing, but that start to proliferate and That you can't kind of pull back afterwards. It might have other effects. We haven't studied well enough. There'S that kind of risk. There is much more acutely the um there's the risk that we're developing things that we know aren't safe for release, but they get out because containing tiny microorganisms is a really hard thing to do. Um and so uh. Just accidents are a very real thing and then obviously uh bio weapons and not just state actors that you can kind of deal with via deterrence. But non-state actors, where it's very hard to deal with them via deterrence because they might not care and or might because they're a suicide cult or whatever and or because they're not identifiable. When we see increasing kind of suicide shooters, where you know they can kill a lot more people than assault rifle than they could with a handgun or with a knife, as we move from assault rifle to weaponized drones or biotechnology or whatever, obviously, how much harm a Psychologically unwell person can cause as the decentralized weaponizable technology increases changes a lot so far, there's been this fortunate thing that the people who are good at tech and who are good at strategy have mostly been gainfully employed and not in the place of want to burn Everything down as we're moving to a place, whether it's through things like uh, radical polarization, where both sides feel that elections are existential and they feel that there is some kind of grand force uh harming the world irreparably or whether it's through way more people being disenfranchised. Through things like uh climate refugees, um and through the effects of kind of the social media orientation, as it is optimizing for engagement, that is largely achieved through getting people more limbicly, hijacked and tribal and um doubling down on confirmation bias. The total number of people that feel existentially disenfranchised and are motivated to take some kind of action like that that intersects with the number of people that would have catastrophic capability. Those two circles are moving closer together. So there's more in the venn diagram and the more people with catastrophic capability is also growing exponentially. So that's a increase in the vulnerability of the world. Um, i think i think, in his conversation with sam harris robbery, talks about one of the examples of concern being a project that is happening at the time of filming this. To try to that. Actually, a u.s government department is engaged in trying to get all of the bio labs that do gain a function: research to publish all of the sequences to some kind of open source database so that the knowledge is more available for preemptive vaccine creation, so that, if Some future pandemic occurs. We'Ll already have vaccines in place totally understandable. To want to do something like that to want to take the collective knowledge that humanity knows and and centralize it so that um, so that we can coordinate on solutions like it seems like a good idea, though, of course that also means making more centrally accessible, and Maybe even open source access to all of the catastrophic bioweapons capability for anyone who would use it for bioweapons purposes and in a world where the technology needed to build that is getting increasingly easy. That'S just an unbelievable risk. So this is again an example of the way we try to solve. A problem can make way worse problems, and so one of the things you want to think about is what are the possible negative externalities of our solution? Whatever our solution is one, what are the? What are the upstream causes of the problem we're trying to solve and have we address those upstream causes? If not, what do those upstream causes do when this isn't their outlet and what are the possible negative externalities of the success of our solution? We have to get so much better at those models of thinking. With regard to ai, we can see that the ai curation algorithms, that run social media and search engines and infotech input technologies generally are already having a civilization, altering culture altering effect that is particularly heightened in democratic societies. You know this. This is the work of tristan harris, and people like this got plenty of stuff on the internet, for people to check out. If you haven't seen social dilemma, but briefly that the ability to take kind of all of the information to say say we're talking about facebook. You'Ve got billions of people incentivized to upload stuff incentivize by getting likes, and you know whatever and that's to create their own content or curate existing content, and so you've got billions of people creating content, billions of people, uploading content and then billions of people engaging with That content and where what is going to show up in my news feed, is what is optimized to make me, spend the most time on site and engage with the feed the most empirically, as it exposes me to different stuff in the news feed and it empirically Measures what i would engage with the most - and it happens to be that uh consciously - i probably did not plan to spend an hour on facebook today, as i was planning my day, i probably wanted to check it briefly and go to other stuff. So if i stay kind of an executive function, i'll probably get off facebook. If i get limbicly hijacked through um, you know a bunch of hyper normal stimuli, airbrushed pictures and whatever and things that outrage me i'll, probably spend more time on site. If i get confronted with views that are complex and make me think i'll, probably bounce, if i get things that kind of appeal to what i already think and enrage me i'll, probably do it, the clickbaity titles get clicked on more so without trying to there is This kind of comprehensive appeal to the lowest angels of our nature that occurs through an ai right. It is a advanced, artificial intelligence that is doing curation algorithms. It is making almost everyone more certain, more sanctimonious, more righteous, more tribal, uh, less open-minded, more villainizing across all the views. It'S not left, aligned or right aligned or whatever it's engagement, aligned for everybody. That'S already an example of catastrophic risk happening globally. Then an increasingly polarized population elects an increasingly polarized representative class. The increasingly polarized representative class in a democratic system means more gridlock and inability to do anything which means inability to regulate exponential tech or solve climate crisis or environmental crisis, or engage in the great game of geopolitics with china. That doesn't have the same issue, and so we can see like real, true catastrophic risks at the level of the environment and geopolitics as a result of the ai that is already employed in what seems like benign entertainment, you know channels or whatever and that's extremely low And that wasn't intentional right like it was not designed to destroy democracy. It was designed to you know, do the very basic functions of design. Do those are second order effects as we look at things like gpt3 t3, that is already able to generate novel text that passes the turing test, that people read and think it was a human that wrote because it's good enough and can already generate novel text of Specific types like in the voice of a particular person on a particular topic by in in taking all the writing of that kind and again, though, the rate of growth of generative ai of this type is faster than moore's law. Right now, which both means how powerful it is and how widely available it's becoming so a lot of people have started to think about the kind of deep fake apocalypse of when content can be generated, not just now curating existing content, but generating bespoke content. That can be used personalized info about people or groups to put forth content. That would be maximally appealing to them that can utilize statistics and images and whatever, but that are all created. How does anyone make sense of anything in a world where i can have more fake information than real information? Can'T tell the difference and the fake information is more compelling. Now you start to imagine: you have the curation algorithms and the creation algorithms working together to be able to both show me the best of what has been created by humans and and then creating specific content and creating those together. So you can see like a breakdown in public sense, making tribalization polarization those types of issues associated with ai and that's again, that's like happening through market dynamics already almost past tipping points, this isn't getting into things like autonomous weapons. But as you start, you can also, of course, look at. We are in a kind of global arms race on lots of different new technologies. Every new technology makes a weapon and nobody wants to uh not develop the weapon if somebody else is developing it. So, if anyone's going to develop it, we need a race to get there first plus counter weapons. So there are arms, races on biotechnology and ai, and so um autonomous weapons that you know can make autonomous decisions and have lethal capability creates a radically more vulnerable world. For everybody and nobody would think that's a world i want to live in, but nobody feels like they have the choice to prevent it, because someone else is going to do it. That'S an obvious example, but even like the swarming algorithms, the kind of image recognition, ai and swarming algorithms that can take a few commercial drones and make them swarm together, is a type of ai that is took very, very advanced development at places like mit to make. But is now downloadable for free on github, where any maker, with some basic capabilities can do that and the total number of infrastructure targets that could really damage the world if hurt that are vulnerable is high. So there are heaps more examples in general, if you think about say, alphago, google's deepmind's technology, they can beat the best human player at chess and the best human player at go and the best human player at starcraft in any definable game. It'S already not just a narrow intelligence right, it's already a narrow intelligence across any sector that you want it to be. That has radical supremacy over humans and the speed at which it develops. The capacity to do that is is mind-boggling, and so, as you think about, could a game be made of turning lots of metrics that you'd want to optimize for into games and figuring out how to win those in terms of applications in militaries and markets and public Opinion and whatever uh that um that technology is in a it's, not an arms race between nation states. So that's also happening. It'S a market kind of arms race between companies competing for getting there first um, but there is definitely an all-out race to advance the most powerful technologies of intelligence. And obviously you have a artificial general intelligence case where it moves completely out of our control. And maybe it's not possible for us to create alignment between its intent and ours and that's kind of the biggest risk, but even the one where it stays within our control, but it is radically leveraging human choice. This is one of the things when we say we've got the power of gods without the love and wisdom of it. History doesn't show humanity being particularly good stewards of its technological power in any meaningful definition of good. The that we use increasing technological power to increasingly exploit the environment, other classes, other people, it's pretty clear and uh. What i would say is that we don't get to continue to utilize the power of technology in those types of ways, as exponential technology comes online without self-terminating, and so the type of mind that gives us the ability to make the tech is not the same type Of mind that gives us the ability to regulate it and steward it wisely and that type of mind being able to catch up with and guide direct and bind. The other is critical to humanity, making it through its technological, adolescence. Okay, all right. So this brings us to the topic of multi-polar traps. This is a underlying feature of what drives many of the major problems in the world, both in terms of market issues, environmental issues, military issues, many things like that and without being able to solve this underlying feature. More fundamentally and categorically, none of the specific areas find adequate solutions. So i want to make sure we understand this. If you want to read up more on it, there's a exceptional paper online called meditations on molok on slate star codex. That'S probably the best overview on the topic, i'm aware of so what do we mean by a multi-polar trap? We mean a multiplayer prisoner's dilemma or a situation in which you've got a number of different actors that could be different. Nation states different corporations, different tribes, whatever it is uh who can be in a competitive dynamic with each other, where, if any of them do a particular type of action that, if everyone does, that will kind of create the worst case for everyone long term, but will Create so much advantage for them in the near term that they will win enough power that everybody else loses if they don't also do the thing, and so, if anyone does the immediately advantageous, though long-term harmful thing, everybody else has to race to that thing. So let me give some examples: uh say there is a new type of technology that emerges, that has the capacity to create a more powerful type of weapon, and if anyone starts advancing the technology for weaponry in that way, they will win the next war. As a result, everybody has to not only advance that weaponry, but they have to try to race to advance it more powerfully than the other guy faster and also a whole suite of other weapons that are like counter weapons and defenses against that thing. This, of course increases the lethality of the entire world in the lethality of the next war for everybody, but if we can't prove that the other guy isn't going to do it, then we all have to race together as fast as we can - and this obviously happened With nuclear weapons, this is even when we were supposedly doing nuclear disarmament. Nobody wanted to give up their last nuke, which is why nuclear disarmament is so hard. Nobody wants to give up their last nuke first, because what? If the other guys say they gave it up and they didn't really and they actually have a nuke and some secret deep underground military base, and then it's game over, for whoever authentically gave the thing up. If we don't have the transparency to prove that the other guy really gave it up or hasn't done the thing, then how can we ever create those agreements when there's so much consequence on the asymmetry of capacity in that agreement? So, even when we were supposedly in nuclear disarmament, there was still an arms race happening to make faster and faster warheads the hypersonics, because if someone had nukes that were way faster, they could win first strike, and so we see these types of arms races. We see this right now in all the categories of exponential technology, a new type of computation or computational capacity comes online. It leads to new types of cyber weapons and there's both a race on the development of the cyber weapons and a race on the ability to create defenses and hardening against that. The same is true with biotechnology, as we mentioned earlier, that there are real existential risks associated with both um with many types of biotechnology, from genetic engineering to synthetic bio, and many of these are not from weaponized purposes, they're from positively intended purposes that have externalities or Positively intended purposes like gain of function that have accidental releases, but obviously weaponization is one of the possibilities so again, if they portend so much power. If anyone might possibly be doing it, then everyone has to race to do the thing and the counters to it, and probably this is nowhere more pernicious than in the case of artificial intelligence and because artificial intelligence increases the ability to do every other kind of weapon And you can use artificial intelligence systems for developing biological and chemical and cyber and every other kind of purpose and there's this concept came from john boyd. Originally the concept of the ooda loop observe orient decide act. The speed at which a particular actor in a military conflict can observe what's going on orient to it, who's the bad guy who's, a good guy. What'S the right thing to do, make a decision act and then do it again, based on the consequence of the action, the speed and accuracy with which they can do that determines who wins in highly unpredictable scenarios. Obviously, ais are going to win ooloops, and so then you have ooda loops in kind of multi-polar traps with each other leading to the development of maximally capable ais and kind of multi-polar traps with each other, um and you'd say well. Why not just make an international treaty that no one will build ai weapons? That seems very straightforward. The u.n could mediate it and we could all just agree. Nobody build ai weapons because really, who, like a general, still a person with family, maybe kids grandkids like who wants to live in a world with ai, empowered autonomous weapons that have that speed of decision making and lethality like it's just a up world. Nobody would want to live in that, but because we can't ensure that no one else is doing it, because, even if they say they aren't doing it, how do we know for sure in some deep underground military base they aren't doing it? We have to assume that they are and then we have to assume that they are doing the most advanced version possible and then we have to still ensure that we beat them at it and then, of course, they also might want to not do it, but they Have to assume that we are so. This is a collective action problem, a coordination problem and we can see the application in arms races now, as we mentioned earlier up until world war ii in the nuclear bomb. Every time there was a more advanced weapons technology, there was a race to implement it um for the advantage it would give with nuclear bombs. You had a situation where the destructive capability could be omni-destructive, that you had a race to develop more and more potential capacity to use it while trying to ensure that no one actually used it. It'S a weird situation, we're still in the place of trying to develop the potential capacity to win just through deterrence threats. You know whatever it is, but obviously our the total lethality and the total consequentiality of what could happen if initiated, even because of accidents, especially in a time where launching disinformation campaigns and cyber attacks on proper information of uh. If someone else launched their thing or not, is getting increasingly easy for for non-state actors, it then affects everything else: the precarity of that much destructive capacity in that many locations, like it's just obvious right, just to think about the precarity of even nuclear weapons. If you haven't read the doomsday machine by daniel ellsberg, it's really worth reading to see how many times nuclear weapons almost fired, just because of accidents like person, accidents or computer glitches, and it's almost it almost inspires some sense of like supernatural awe that we are still Here but increasingly so, as we think about how many more catastrophic capabilities and how many more hands are emerging, so the multi-polar trap is underneath. Why we can't say: let's just not do a particular thing, and we see that in this kind of military example. Here you can take that all the way back to kind of early tribal warfare, and let's say that there was a area where there were some tribes, that uh were actually quite peace, loving and not oriented towards warfare and maybe had some kind of animistic or spiritual Ideals to want to live in harmony with nature or whatever it was as much as they could if any tribe around them starts being oriented to tribal warfare, realizing that the increasing population in the area is creating a competition for the dwindling amount of natural resources available Fishing or hunting, or whatever it is, and that killing people in another tribe means decreasing the competition for those resources plus getting the stuff and the surplus they've already created, and that the weapons you use to hunt and the weapons used to go kill. Another tribe are not that different if any tribe orients towards that every other tribe has to orient towards that or they lose by default. You don't get to just say: hey, i opt out of this game right, and so then that creates a situation where, if some group orients towards that very powerfully right, like genghis khan or the spartans or whoever, who are going to invest most of their capacity in In military tech, how does any other culture in the presence of them not get destroyed if it doesn't develop adequate military tech to at least defend itself? And how do you defend yourself against someone that is that exceptional in that without having that change? Who you are as a culture completely, because you have to now create all the investment in that category? So when we were talking earlier about kind of long arc of history, the idea that um warfare is one of the selection environments, where cultures made it through or didn't make it through, and the ones that were not successful at warfare in relationship with the ones who Were waging it didn't make it through, so there was a selection both for the capacity and orientation to be effective at it, and then the more the more successfully violent ones took out the less successfully violent ones. The successfully violent ones warning with each other upregulated each other's capacity in a particular kind of you know multipolar trap scenario, and then we are the descendants of those who made it through that um. It'S an interesting and important insight. You can see that for a couple hundred thousand years give or take of homo sapien history. We mostly lived in these very small tribes. You know below the dunbar number, because people we we can assume didn't want to live in much larger groups. Where for the group to have coherence, they would be bound by certain rules that if the group was too large, they didn't have a saiyan. When you are below the dunbar number, everybody can sit around a circle around a fire and all have a say on a big decision. So if i'm going to be bound to something i at least want to say in it, i want, if i'm going, to sacrifice for other people that they're people that i really know as soon as the group gets too large, i'm supposed to make sacrifices for people. I don't know, and i'm bound by rules - i don't get a say in no. The group would rather just cleave and make smaller groups. It seems clear that uh humanity stayed below the dunbar number for whatever set of reasons pretty rigorously for a very long time, and so that's very you know it's very interesting from like the scale at which humans evolved to be able to operate like our whole. Evolutionary genetic history was operating at that kind of scale mediated in in those ways, but of course, amongst other things, one of the things that would have selected for ending that would have been tribal warfare and, if say, a larger tribe. Uh wants to initiate warfare, smaller tribes, better merge together to be able to defend themselves, and that means probably giving up some uh say and freedoms in um and quality of life so as to have safety and security and now, of course, the race for larger groups. With more effective weaponry and more division of labor to have more, you know, capacities and more surplus, which means more extraction from the environment that race is on, so we can then kind of see the history of civilization in terms of the history of extraction capability from Agriculture and mining, and whatever the um, what we would call um exploration capability, which means being able to convert more of the natural world into stuff that we use the expansion of military capacities, the expansion of coordination technologies, the expansion of um. You know size of group that can be in coherence with each other to fight rivalrous dynamics externally, and so you know, eventually we got empires and kingdoms and the nation states and then kind of global economic trading blocks and nato's, and things like that uh with the Exponential increasing types of technologies that we have both for extraction, you think about a mile-long drift net being able to pull uh hundreds of thousands of pounds of fish out of the ocean in a single go compared to a fishing line or compared to what an orca Can do this is exponential extraction right and we can see that in every area of industry, from factory farms to mining, to drilling, to fracking, to whatever exponential extraction, exponential monetary creation, exponential information processing weaponry, et cetera, but where there is some externality to the environment and To social cohesion and to um sense-making happening everywhere, you're advancing some things causing externalities, but at you know, exponential, increasing scales. So historically, what has won and made it through was an increase in the capacity to win a game theory, and yet that is kind of a long exponential curve that is really verticalizing now and it's verticalizing in its total game theoretic, which also means externality, creating and Destructive capacity while reaching planetary boundaries, and so if we continue to do that, which is always one which means win at arms, races that process itself self-terminates, and so this is a real tricky thing, because we can neither say well, let's kind of do the luddite direction Of like less advancing military attack or whatever and just lose that doesn't work, we don't want like uh. What happened between tibetan china has happened a lot of times throughout history and trying to do. Tibet doesn't work right, but trying to continue to win at game theory. Driving arms races that are increasing destructive capacity and externalities also doesn't work as you're approaching the point at which that creates inexorable, catastrophic risk, so something that has never happened before has to happen soon. There are some other examples of multipolar traps. This is an example of kind of the arms race side. We can think about the tragedy of the commons as another, multipolar trap, and it's not that no multi-polar traps have ever been solved or bound. They have. You can read eleanor ostrom's work. You can read the process of commons management. You can obviously see how mutually assured destruction was binding, a multipolar trap on nuclear weapons in a particular way. It'S that we don't have adequate solutions to the nature of the multi-polar traps that are catastrophic, that we face currently so rule of law is a way to bind a multipolar trap right inside of a nation state so that everyone isn't racing to cut down all the Trees and there's no national parks, we agreed to make national parks and then create a monopoly of violence, which is a police force to be able to back that up. So if people try to go to do it, they, you know be physically forcibly stopped until we get to have some trees, um, and that would also be true for crime and pollution, and you know other things like that, where we use law and the ability for Enforcement of law to be able to back it up within a nation state. There are places where that's really hard to work, because of course, then the economic incentive is to try to capture the regulator. The regulatory apparatus is trying to bind where there is excessive perverse incentive in the market right. The idea of kind of a liberal democracy is, let's, let's let the market do most things, because it's kind of a decentralized, collective intelligence where, hopefully, people demand real goods and services that will benefit their life, which creates a in evolutionary niche for people to create products And services as supply and to compete with each other to make the very best product or service at the best price, so that the rational actor will buy that that's kind of like old-school market theory. This is obviously not true. It has some truth in it, but it's not completely true. As the behavioral economist showed, people are not rational actors who make the the best choice, especially in a world where nobody can even see all the choices. There'S a kind of information overwhelmed, so uh. The thing that actually is the most effectively marketed thing will end up succeeding over the most effective thing. Uh manufactured demand is very easy, so, rather than demand driving supply, people now want that won't actually increase. The quality of their life will empirically make their life worse that they never wanted before, because the nature of marketing manufactures demand that wasn't there. The whole idea of the collective intelligence of a market is that demand drives supply. Like that's a foundational idea, i would suggest as to why it is actually net good the moment that the supply side can drive demand for things that don't actually increase the quality of life meaningfully, which is through largely appealing to addiction and keeping up with joneses and Kind of lower angels of our nature, stuff and, of course, as soon as corporations start to become large, which earlier, if you think of like a local market that wouldn't have been the case. But as soon as corporations become very large, then supply and demand have a radical asymmetry. The of course, supply and demand should be the same size in aggregate, but the supply side of a major corporation, a billion-dollar corporation, is coordinated as a corporation with a org chart and decision-making processes, and all of the consumers are not in some kind of like labor Union for consumers, some consumer union that give it has equal information processing to be able to play that game theory. It'S the multi-billion dollar corporation against the individual in terms of the game theory. So, of course, the corporation can employ behavioral psychologists and ai split testing and all kinds of things to be able to optimize supply driving demand, rather than the other way around. Underlying intelligence of the market is broken at that point, and you really just have asymmetric power. Having captured the ability to maintain and advance its own power, but so the idea was in kind of formation of modern democracies. Market should be largely free because it does a lot of good decentralized things. We want to decentralize stuff as much as possible democratize decentralized, but the market will eventually create a power law distribution. Some people will be better at market than others and as a result, they will make more money and they will be able to use their more money to make more money and so you'll end up getting rather than an equal distribution of money. A pretty tight power law, distribution of money and then the people with the money have maximum control and it'll become feudalism again and we're trying to get away from feudalism. So you have to make something more powerful than the most powerful wealth class. If you don't want it to be feudalism again, realizing that the wealth will follow a power law distribution. So the idea of let's make a state that has rule of law and has a monopoly of violence so that it is more powerful than the most powerful of the top of the market. And then the state is run democratically so that the collective values of the people and then, of course, this only works if the values of the people are somehow developed. So all of the kind of founding documents in the us and other modern democracies talk about things like the number. One aim of government should be the comprehensive education of every citizen in the science of government, george washington, or things like that - that it requires a comprehensive education, informedness and some kind of moral education simultaneously for a democracy to work. Otherwise, democracies are just a really dumb idea of lots of rivalrous, uneducated people all having a say in how things go about that they don't know anything about. Socrates is kind of critique of why democracy was a dumb idea in ancient greece. So if you want to be able to democratize decision, making, you've got to educate and enlighten the people, i think it was franklin or jefferson's quote on if you believe who should the ultimate uh depository of the power live with and if you believe the people to Be too unenlightened to hold it, then the obligation is to enlighten the people, because there is no other force that can hold it. That doesn't become despotic. These are paraphrases, so the idea is uh a civilization. You know our society that invests in the collective human development of the people that the values of the people then get encoded through some kind of democratic process into rule of law. The rule of law has a monopoly of violence so that it can then bind the predatory aspects of the market, because mostly things that are against the law are things where somebody has an incentive to do something, but that we collectively agree. We shouldn't let people do but there's an incentive to do it, which is fundamentally a market type force. So let the market do a lot of things regulate the up things. Of course, the challenge becomes that the market, where it would be regulated by the state in a way that is disadvantageous towards it, has its own incentive to try to change how the state regulates and so uh. This is where lobbying comes in. This is where campaign finance comes in, and this is where you know all of the ways in which regulatory capture by uh market starts to come in. And when you realize that the people in market positions have an enduring in incentive to the market where the people in representative government positions don't have an enduring um loyalty to that position, this is the public choice critique of representing democracy. It becomes pretty easy for regulatory capture to occur, see heaps of examples of that, but so there there was clearly some right ideas in those structures of what should be done through decentralized type process. Where does that still have failures? How do we figure out how to create some centralized power that still represents the decentralized will of the people to be able to create checks and balances on power coming out of the problems of uh monarchy, not having the no bless oblige that it was supposedly supposed To have the modern systems were very much developed around the idea of checks and balances on power, so don't allow too much power to concentrate anywhere as that ends up having you know, uncheckable corrupting orientation. So let's create a separation between the market and the state. Let'S create a separation between the state and the church and other kind of civic organizations, let's make sure there's no monopoly on religion, so all of the religions are allowed, let's make sure, there's laws against monopoly on corporations, so the competition between corporations can check each other. Let'S split the government into three different branches: um you know on and on right this all designed to be: how do we create checks and balances on power so that abuses of power? Because, where people are acting harmfully with symmetrical power, they can kind of check it themselves where someone with asymmetric power over someone else is acting harmfully, it's very hard to check. So how do we create checks and balances? There'S a lot of right thinking in this, but total advancement in technological capacity and population size and many things have made it to where those systems, as they were, put in place, not the principles, but the systems have mostly broken. If we were trying to build a open society, a kind of democratic or participatory governance system from scratch today, in the 21st century, we wouldn't be thinking about people meeting physically in a town hall that can't possibly hold the population of a local area to have a Representative, who has to ride a horse to like we wouldn't be thinking of that we'd, be thinking about how to use the internet to create town halls big enough that everybody could fit in we'd, be thinking about since there's too much information to process we'd be thinking About how to use artificial intelligence to process the everybody's views into forms, information compressed forms that we can actually work with. So this obviously has to happen. How do we take the advanced technology that has fundamentally changed the nature of governance and implement it to build new governance systems, aligned with similar principles and evolutions of those principles that we've gained since then mediated through the right capacities? Currently, the idea that rule of law an estate is a way to bind a particular kind of market-driven, multi-polar trap, but that is, of course, within the domain of the rule of law within its jurisdiction. So then, international tragedy of the commons issues are a different kind of thing, because where does the enforcement live becomes a trickier issue? So if we think about uh tragedy of the commons issues that operate at a international and particularly at a global level, we can see that the world is having a really hard time figuring out. How to deal with these climate change is a very classic example, because every place is utilizing. I mean every nation is utilizing hydrocarbon fuels and given that energy use in gdp correlate and everybody's in a multi-polar trap race for gdp growth, because that equals increased optionality for everything else, which also means the race to use more total energy and which is driving climate Change, how do we get out of that? Well, obviously, we aren't paying for the real price of energy right, we're simply paying for the cost of extraction uh. My friend, nate hagen, says really great. Work on this topic has a very good podcast. We'Ve done some discussions. There, if you want to get more into the issues of energy ecology, but we basically pay for the extraction of the hydrocarbon like what does it cost to mine this barrel of oil and then add some little margin to make a profit? But that's not the cost that it would take. If we needed to make those hydrocarbons ourselves, would it cost nature to do it as they're, fundamentally unrenewable or the cost to the environment of the pollution side of it? If we were trying to cost appropriately what it would cost for us to make that fuel renewably and not cause environmental externality in the process, the price would go up so much and then that, as a key input to every industry, that markets would, as we understand Them today fail in every sector. It'S a big deal like because that's real but we're incentivized to externalize all the costs, because other people are and if we don't, we will lose in comparison. So an incentivize to use all the energy we possibly can to grow the gdp as much. So if we try to make the price of carbon more real, we try to drive the cost of carbon up to disincentivize the use of it. We so radically disadvantage ourselves in terms of gdp growth that if anyone else, if any other major player, doesn't also do that, then they will gain so much geopolitical advantage military advantage. Population growth advantage that they will simply win this multi-polar trap. So unless we could get all of the major players to agree to the thing and have the kind of transparency to ensure that they actually were keeping the agreement and then have some kind of enforcement mechanisms where, if they didn't, we could enforce it. And this is also really tricky, because how do you enforce adherence to something of another country that also has nukes right like what? What what are you? How far are you willing to escalate the thing before you're like now? We'Re just actually not going to use our nukes on you. So if we do a sanction and you don't back down - and so then we do some limited military action, you don't back down at some point, you just got to say: okay, we just don't have the ability to enforce this. Therefore, we have to race, because if we go all the way in enforcement and you go back we're all so um so transparency can we see if the other people are keeping the agreement or not the ability to create those agreements. The transparency to see if they're being kept in the capacity for enforcement are critical things to be able to bind multi-polar traps at international levels. So, whether we're talking about overfishing of the oceans or the dead zones in the oceans or the depletion of topsoil or the depletion of pollinators or climate change or any of those issues that advantage us to continue to externalize the cost to the commons in the near Term relative to others also doing it and it would disadvantage us not to unless we can get over the multi-polar trap. We don't solve these issues, so this is a kind of environment or tragedy of the commons case. We already gave a kind of military arms race case i'll give another example of a market case uh, which is basically the race to first mover advantage of new technologies entering the market. The race to market dominance network dominance, as we mentioned earlier, there's kind of a perverse game theory, and this is underlying the game theory of multi-polar traps that those who are oriented to advance a game theoretic opportunity will get a lot further ahead than those that are Oriented to prevent a harm, particularly those who are oriented to advance an opportunity for themselves as players and willing to externalize the harm cost to the commons. They'Re not responsible for will get much further ahead than those that are willing to bind their own game. Theoretic advantage to prevent the harm occurring to the commons and now we're back to the. How does the peaceful tribe not die in relationship to the warring tribe without becoming a warring tribe topic, and so, let's say we're looking at the advancement of new technologies that could be radically destructive to the world? Many different ways like let's say, particular types of artificial intelligence. If i don't really, if, for whatever reasons, whatever cognitive dispositions or whatever, i don't, really pay attention to the risk, or i don't buy it that much and i buy the opportunity, i'm going to do better in being able to advance the product quickly, sell it to People, you know et cetera than the person who's paying much more attention to the risk, even if i buy the risks. If i want to ensure that i'm advancing it in a way that cannot cause those risks, i'm going to move a lot slower and do a lot more steps of protection and safety and due diligence for the commons. It still means i will lose first mover advantage and specifically when network dynamics are at play, network dynamics are very important. Um, we look at say: metcalfe's law is the entry to understanding network dynamics. This is where the value of a particular company or technology, or whatever, is mediated by interactions between customers or users. In that platform, the more users that engage with the technology, the more the value, goes up. The value actually goes up more than linearly. It goes up by a second power to the number of users, so let's say we're talking about something like a search engine or a social media or a um currency. I don't want to use a currency that, like 100 stores use where then i have to have like 30 different currencies in my wallet, that's pain in the ass. I would like to be able to use a dollar that everybody takes um. I don't want to have 50 logins to different social media accounts that i have to remember where only a few friends are in different ones. I'D like to be able to go to a place where everybody is so there's, obviously so much advantage to the number of users on the network being able to share value in value exchange that you end up getting a situation where, once any particular technology or, let's Say implementation of technology, which is mostly as companies gets to enough market penetration, there's kind of an escape velocity where they will then gain more market penetration faster, and this is why i believe, in the early days, paypal was actually paying people to join the platform right Because it made sense, they got the amount of money that they had to give in coupons or whatever to join. The platform was tiny compared to the increased valuation per user. They followed a second power equation, and this is why there are not lots of fintech platforms of comparable size and that's why amazon is bigger than all the other online stores youtube: video players, facebook, social media, google search etc. So this is a kind of monopoly right. Fundamentally, it's kind of monopoly, but that is not the kind of monopoly that we've historically thought of as something that like was a monopoly through government participation. And so we have not uh oriented towards how to actually bind this thing very well, and it can happen very quickly as we could see. You know facebook and google and youtube achieved fundamental monopoly status in a very short period of time at a global scale, or at least a western world scale. That'S a big deal. It'S a big deal to realize, because then the idea of checks and balances of power within the market is kind of gone right, and so in a world where network dynamics are a thing, then there is an even more intense race for first mover advantage and as Rapid growth as possible, because if you don't get to that escape velocity, somebody else will, whoever gets to that escape velocity, will dominate everything for a very long time, so that just the network dynamics intensified the market races radically. So, of course anything that would slow down. My speed to market and my speed to user adoption uh decreases my chance of being the top of the power law distribution and no other position really matters right. So how to just race in that way is dominating market dynamics in most areas of emerging technologies, and particularly, you know, exponential technologies. So this means ai being developed for a positive purpose that has negative externalities that either nobody's thought of, or nobody feels like they can do anything about it. We hope we'll just be able to solve later, being developed for a positive purpose that can be repurposed for negative capacities being developed for a hopefully positive purpose that might become autopoetic and take off in ways that are totally uncontrollable, etc right. Where, then, of course, our speed of advancement with that drives the arms race of everybody else's increased speed of advancement? This is also a multi-polar trap. It'S a multi-polar trap in the market and, given that now these competitions are not just at the level of a local market, but all at the level of a global market, then the total speed of the race and the scale of the competition and the scale of What is affected is just its zenith, so in the area of the environment and tragedy of the commons and the particularly in the international cases where enforcement is difficult in the area of actual arms, races of kind of military technology in the area of market races, where The race to first mover advantage of market dominance ends up being a race to the bottom in some important way. These are all examples of a multi-polar trap or a situation in which the inability to get out of the perceived competitive, dynamic means that if anybody does the up thing and since you can't prove that they aren't doing it, you have to assume that they are means. Everybody has to race to do it as fast as possible, which means a uh, a globally worst case scenario across all these axes, for everyone, with each actor doing the rational choice that makes most sense to them in the context in the moment. So this is not a particular person being bad, it's like well, it makes sense, of course, if they are possibly building the ai weapons and we don't want our people to die, we should build their weapons, so you have a situation where each person can do what Seems like the obligately moral thing, and yet everyone is collectively doing the stupidest thing for the whole possible, and this is a kind of collective action. Failure where our inability to create relationships of trust and our inability to coordinate, creates not just inefficiencies and duplication, but radically destructive orientation on what could be otherwise constructive capacity. This is one of the uh yeah, most kind of fundamental dynamics, underneath most of both what is driving the catastrophic risks and what makes them so hard to solve. There'S also a question that could be an orienting question for humanity of how do we progressively better bind and solve multiple traps, particularly the ones that are driving near-term catastrophic risks, and this is an area that i think actually has a lot of solutions. Not a categorical or perfect solution, but a lot of solutions uh and that i think a lot of humanities, collective, innovative intelligence focused on would make a really huge difference. For instance, we said one of the challenges of solving the multiple trap. Internationally is the inability to make international agreements and one of the reasons where either we don't make the international agreement, because we it's not even worth making because we're sure they're going to defect on it. So why even bother - or we make the agreement knowing that they're going to defect on it and we're going to defect on it. But we're going to say that we're keeping it and we know they're going to say they're keeping it we're going to spy on them. And we're going to lie to their spies and all this kind of waste that goes into that uh. If the transparency to know what they were actually doing and them to know what we were doing was there the ability to make and keep the agreement would be. Fundamentally different, so underneath multipolar traps is the perverse game theory to orient towards opaqueness rather than transparency. And this is, you know the whole nature of how many things are considered. Trade secrets in the market or national security secrets - and you know all of the different types of security clearance and classification and special compartmentalized information and whatever, because we're trying to have information advantage. Information asymmetry advantage on the other side and we don't want them to know what we're doing, but we do want to know what they're doing, and so we invest a lot in this. But the opaqueness means that the agreements can't be made. So the multi-polar trap is the only thing that is left. So um, let's say: there's a company called planet labs. It'S one of the largest private satellite imaging networks in the world and they've already been working on this for different applications, and we've talked about more applications that could happen. Let'S say that we take a particular kind of problem that drives a global, multi-polar trap that can be seen on the surface of the earth. We don't have to deal with the deep underground military bases. Yet so, let's say we're talking about um streams of pollution that are flowing into rivers, or things like that. If we're talking about real-time video imaging at pretty high of the entire surface of the earth, pretty high granular detail, can we see where the mining waste is being dumped? Can we see where the uh major plastic in the ocean or the dead zones in the ocean where the affluent is coming from by kind of reversing the time sequence on the images yeah? You know, satellites are one example of sensor networks, but they can operate in that way. Does the ability to do attribution right, specific, verifiable attribution of where a particular harm is coming from? Does that increase the capacity to create justice to be able to bind it totally? So is it possible that certain types of forced transparency can make it to where the ability to hide the harms that then have everyone race to do the harms goes away and then the need? The fact that they can be shown means there's a need to account for them and better methods of accountability emerge. I think there's a lot that can be done with forced transparency that orients towards a fundamentally better uh attractor of the game. Theory, space there's a very interesting question. I don't know the answer to this, but i am intrigued of. Are there also scenarios where transparency, besides being forced in that kind of way, like satellites that are looking at the entire world from international space? So the law allows them to do that. Are there ways in which transparent solutions win game? Theoretically, as a different peak in the adaptive landscape relative to the opaque solutions, why the opaque solutions win is very obvious. Surprise attacks help right the ability to advance something where they don't know that we're advancing it. We mislead them about it. They build a false defense, like it's obvious. Why the desire for um information, asymmetry in that way is there. I was talking to someone in national security of sweden, who was telling me some very fascinating things and i haven't been able to verify all of them but saying that the of kind of major nations that have meaningful defense capacities. Sweden has maybe the most transparent intelligence and military and security apparatus, and their underlying philosophy was that the major players are going to be effective with their spying and no anyways. They don't really get that much information, asymmetry, that's more of an illusion of control than a reality of it, and so russia and china and the us are going to know anyways and so the huge amount of resource that they would have to put into trying to Hide from them is mostly a waste, so they're, just not even a try and that if you have to try to hide the national security stuff from the quote-unquote enemy, that also means you have to hide it from your citizens, because otherwise, your citizens, don't all know How to keep classification, which is why, like in this thing, pretty much, makes democracy impossible in any real sense, more than just a simulation at the time of the founding of modern democracies, and particularly i'll mention the united states. Here the united states had an ocean on both sides to anybody else. It had no kind of contiguous um rival, land, contiguous rival. That was a pretty awesome security situation, and so the decisions that needed to be made about anything, including military things, could be shared with the people. The people could actually weigh in on them and sharing that information with those citizens didn't instantly mean sharing it with the british or the spanish or the anybody else, because there was no way to get that information across the ocean quickly and um simultaneously. Once someone else got that information, they couldn't launch an attack that quickly they'd have to launch boats that we could see that would take months. It would give us time to launch a you know, a response, and so the ability to share with the citizens was actually viable in that world because of those sets of reasons. As soon as we get to a world where, whatever is being shared with, the citizens can be known by rival countries instantly because of uh electronic uh, you know telecommunications and where they can then launch attacks instantly, including ones that are hidden with plausible deniability. Like cyber attacks or economic attacks or geopolitical positioning, let alone missiles. Now the threat of sharing information with the citizens, meaning sharing with the rivals, is way too high. Therefore, more and more things become classified more and more things become national security secrets. Obviously, attacks on our water supply or our energy grid or our food supply could all be national security threats, so we start finding that there's national security secrets everywhere. Well, how do you do democracy if the citizens can't actually know what is going on and what the real considerations are? Well, you can't it's it's a simulation of it. This is a real challenge that, like we have to think of in a deep way when we consider what does a participatory governance system in the context of the modern world look like, because it makes sense that there would be a black budget, and it makes sense That there would be national security secrets, but then how do you verify that those authorities are not corrupt? How do you verify that they are actually doing the will of the people or how do you you know? How do you create checks and balances on power or adjudicate issues, or things like that? This also relates to the thing we were mentioning earlier about that, when even scientific publishing equals increasing catastrophic risk, because the scientific publishing creates the capacity for more catastrophic technology simply through information, i don't have to actually give the capacity to manufacture the tech. That is easy. Give the information on how, to code a particular thing, catastrophic ability increases. How do we do open sharing of information in a world where other actors, and increasingly smaller actors, can use that information in increasingly destructive ways? And if you don't, then how do you have anything like participatory governance without open access to the information? This is now why we talk about that. The two primary attractors for the world are catastrophes and dystopias that uh. If we continue to advance exponential technologies where more and more actors have capacity to intentionally or accidentally catastrophic technology and that them like the industrial tech, cumulative effects, the now not just weaponized or accidental immediate catastrophes, but the cumulative externality effects of the technologies continue to advance. So rapidly that world orients, towards increasing likelihood of catastrophic risk to be able to prevent those, catastrophic risks means being able to bind the intentionally or even accidentally or even externality, driven catastrophic potentials. The ability to bind that looks like some very, very powerful regulatory capacities of some kind, whether it's a decentralized network regulation or whether it's a state or whether it's a whatever the whatever it is. And then how and then most of the ways in which we could build a thing that could do that. That thing to have the power to check decentralized exponential attack has to be powerful enough that what can check it. And so then you end up getting dystopic solutions. So a classic example, though by no means the only one is. I can't build nuclear weapons in my basement. It'S too hard to build them, but if i can build bio-weapons or i can build ai weapons or cyber weapons or drone weapons in my basement, that can produce not only really bad harm but possibly cascading harm or illicit other actors to do similar things in the Presence of decentralized catastrophe, weapon capability involving no exotic materials. How do we prevent the world from breaking which increases in probability as time goes on, without something like a system of surveillance? That knows what people are doing in their basements? Well, it's very hard to imagine how to do that. Actually, and yet it's very hard to imagine how a system of surveillance that can prevent that, which is a very understandable thing, to want to do an important thing to want to do who? Who has access to that surveillance? How do the issues of determining right use of that surveillance and the power of that get adjudicated very easy to see that becoming a really dystopian big brother scenario already in the west? Many people feel that the approach that the chinese government is taking to address very real issues that they see as kind of catastrophic to their civilization that we're not addressing well enough and are eroding our civilization. We see as dystopic to certain kinds of civil liberties. We can see the way that it's clear that the polarization engine that social media and infotech is that is driven increasing polarization uh in the united states. In particular, china resolved that very easily by having their central government just start to ban and regulate those technologies right. That kids only had access to a subset of tick tock that shows patriotic and educational videos, and they have a limit to the amount of minutes per day and per week. They can play them in video games and no live streaming and like they were they. This is a huge deal now, that's totally a you know something we could call a paternalistic like overreach of state, but we can also see that our state is eroding by not doing that and being captured by market forces and broken down by polarization forces and the Sesame credit system and the iot, the ai mediated iot kind of surveillance system makes a lot of sense to prevent catastrophic technologies happening decentrally. It also creates a lot of uncheckable centralized power, so we, i would say that uh. When we talk about what is a desirable civilization, it becomes a really tricky topic, because it's like centrally existential questions to say what is a: what is a good civilization. We don't have a system like science. This is kind of the is, ought distinction where science can say what is but not what are we? We don't have a as powerful as sciences for being able to understand the objective world, which then allows us to create tech, objective tech that affects the objective world. We don't have a comparably effective way of studying the subjective and inter-subjective world of being able to say good right. And so, if you have the ability to affect the world exponentially more powerfully through science and applied sciences technology. But you don't have a comparable, powerful force for what is good or wise guidance of that technology. Then what is guiding that technology ends up being game theory right and game theory is actually like a the closest thing to a scientific to something that is commensurable with the scientific system in terms of what is a good choice, which is kind of the social darwinism Of a good choice as a choice that doesn't lose game theoretically to other choices that are also seeking maximum advantage. But we can see that the advance of game theory under the presence of exponential tech in this way self-terminates. So that clearly cannot be called a good choice and yet losing in the near term is also not a good choice. So we need something that is option d, none of the above. So while i think it is important for us to become as good at the type of mind at the type of internal human capacity, that can do wisdom and ethics as we are at the capacity to do science and technology and to be able to think about What is a desirable civilization? I think we can start by agreeing on a couple things that are not desirable and i think that's actually very helpful. I think the idea that a civilization that self-terminates is probably doesn't meet the criteria of what a optimal civilization is, so that we are wanting to prevent the movements towards cascading catastrophes that seems pretty straightforward and that one that is clearly dystopic, meaning that there are such Radical asymmetries of power that uh the freedoms of almost everyone are radically curtailed. Uh is also not an ideal case, and so and yet it's very easy to see that exponential tech has the ability to decentralize power as the market cases, giving an example of which increases, which creates these coordination, uh collective action problems and multipolar traps, and so the Decentralized, catastrophic or the decentralized exponential power creates multipolar traps, creates increasing catastrophes. So then, it also has the ability to centralize power right if i could have an entire nation-state in historically, the nation-state couldn't be too top-down and too big, because it would get fragile because you can't actually see what everyone's doing and control it. But the ability to have sensors everywhere and have no person could make sense of that and you couldn't even trust the chains of command, but ai systems could and then getting everybody to spy on each other is tricky but being able to mediate that, through sesame credit Type systems - i could do that thing, so the exponential tech also creates way more powerful autocratic systems at scale. You know centralizing power, which brings us back to why the multipolar trap is one of the very deep underlying things to work on. Ah - and this also brings us back to the swedish example that i did not complete of where transparency could win so the example he was giving - is that rather than invest any resource in um opaqueness or you know hiding or cloaking stuff, they would just assume that The rivals would find out anyways and, as a result, they didn't have to keep stuff from their own population in the same way as a result, the population has way higher trust than government as a result way, less money has to be spent in campaigning and convincing People of things and there's more kind of emergent coordination and things like that and the various departments of government, because they're and especially of military, have more transparency to what each other are doing because they're transparent, whereas in the classification you don't just have overarching classification, you Have this kind of special compartmentalized information process, which means that a general in one area in a general and the other still don't know what's going on in the other domain, so that both means that duplication can be happening? It means that a lack of efficiency of coordination happens and that particularly when there's too much information, it kind of doesn't even matter to have access to all the information because nobody can process it. This is the infosingularity issue and we'll talk about this more, but when we have more computational capacities to process large information sets, then it actually really does matter having transparent access to all of it. We can make progressively better sense of it with the right, computational sense, making augmentation so their ideas. The transparency regarding things that would have otherwise been classified doesn't lose us that much. It gains us a lot of trust in government which gains us a lot of discretionary participation of citizenry, and it also gains us the ability for the various departments of government and military to be in much more coordination with less duplication and less waste because they have More transparency to what each other are doing. It also means that it's much easier to check corruption because they increase transparency, and so you have more, you know, efficiency and integrity, and things like that and as a result, you lose the little bit of asymmetry of information advantage. But you gain a lot of other kinds of advantages, so it's a different peak in the adaptive landscape. I thought this was fascinating when you share this with me, and so i asked you know, that's cute and all as a country that has eu backing, which fundamentally has nato backing um. You know indirectly, and that is not at the head of an arms race. Uh, do you think something like that could work for, say the united states? His take was, yes, actually could work for the united states, because the amount of capacity uplift that would occur - and this didn't mean he thought it was inactive - the the vested interest against enacting. It would be ridiculous, but just hypothetically as a thought experiment, if that kind of transparency did happen, the amount of duplication that's occurring is huge. The amount of corruption and waste that's occurring is huge. The um ineffectiveness of coordination that could be lifted would be huge. The ability to start getting increased trust in government and as a result having the uh, the democratic and government sector, be more coherent aligned with the military sector, as opposed to the increasing dis discoherence in the government, public sector um, and that even if some first attack Capabilities were increased because of the information sharing of other parties, that our total strategic capacity and response and deterrence would still make it or nobody would mess with that that such a thing could be advanced, and i think this is a fascinating line of inquiry. So if say, more total strategic military capacity emerged out of the transparency than the opaqueness approach, meaning per you know military power per dollar. Then it would actually drive a race to the top on transparency where russia and china would be oriented to try to do a similar thing, because otherwise they'd actually be losing the multipolar trap. That is now a race to the top on transparency rather than race. To the bottom on opaqueness, which is increasing the capacity for international agreement, rather than decreasing the capacity and fundamentally addressing the multipolar trap progressively better, if we think about having some forced transparencies through things like international satellite capability and things that uh and open source intel capacities That kind of mess up the opaque anyways can we make the opaqueness increasingly uh poor as an adaptive strategy, the transparency both more forced and more capable of actually winning. Now we start to get a strategy where, for the first time in history, possibly something wins game. Theoretically, that is also better in fundamental ways regarding its long-term viability, that the culture more oriented to peacefulness can actually beat the culture more orange doorfulness. You know warring without becoming more warring. In all the most problematic ways, it can maintain adequate strategic capacity while being oriented in a way that fundamentally is decreasing the types of pathological competition within the system and as a result and driving races to the top between systems of things that are um more positive. Some and less pathological as a whole. Ultimately, whatever you know, we, i would say threading the needle of something that is neither catastrophes or dystopias and without being able to you, know, wave a magic wand and do enactment at the level of the whole world at once. If any group were to do something, the thing they would do has to not lose to the rest of the world. Not doing that thing not just not lose, but it also has to influence the rest of the world because if let's say we could get some country to do some very enlightened set of practices, it still dies from climate change and ai and whatever so long as It doesn't change what the us and russia and china and other places are doing, so it has to actually be able to influence the whole world shifting, but it has to do it, so it has to basically be able to win in some critical influence ways where The nature of what creates the win, isn't externalizing harm or driving arms races or whatever um, so it has to both be obsoleting, some kinds of destructive game theory, while winning at some fundamental types of game theory. At the same time, like so much, the threading of the needle has to occur, and i think when we go back to the sub dunbar tribes, one of the things that mediated their effective protocols was that there was very high transparency which allowed for uh pretty good Coordination and not having collective action failures within the group, there was no real incentive for anyone to be sociopathic or narcissistic, because nobody would want sociopaths or narcissists in the system and unless you can like people and hide it, that strategy doesn't pay. If everybody can see that you're lying in a small travel scenario, or they can see that you put your trash down or didn't, do your part of the chores you're going to clean it up, you're going to kick it out of the tribe so that the smallness Creates a force, transparency creates a situation in which what is best for the tribe and, what's best, for you, are more closely aligned. I'M not saying perfect, but more closely aligned when the groups get much larger, where somebody has the ability to play people off of each other, who don't know that that's happening, because there aren't enough communication channels and be able to um. You know screw some people over here and then go somewhere else to get a new supply of people and whatever. Then, of course, the ability to hide the effects of what i'm doing i'll create a incentive for sociopathy and things like that, which is why we see that those types of power, oriented, say cluster b, personality disorders or something like three times more prevalent in c-suites and Positions of more power than they are in the general population is that they are adaptive in those environments, so they're selected for conditioned incentivized. So, of course, we can get the high transparency and thus better alignment between the individual agents incentive and the whole in a small environment, but that has never scaled if we want to be able to scale. It are some of the new technologies that allow for certain kinds of transparency and then certain kinds of information processing across larger scale and communication. Could they facilitate better methods of collective intelligence that create that notice, perverse incentives and as a result of noticing them and being able to create accounting for them and externalities, be able to create accounting for them, create progressively better incentives and more capable deterrence to align the Um, motivations of individual agents with the whole better, i believe so. I believe that um, some of the exponential computational technologies in particular that portend kind of the fastest and worst existential risks in many ways, also portend the possibility for better coordination systems, and i'm not talking about ai disintermediating humans and having some ai singleton run the world. I think that's a really bad idea for lots of reasons i'm talking about ai being able to facilitate human, collective intelligence, ai amongst other capacities, but where i don't want artificial intelligence making the decisions by itself. In most scenarios, i want processes of collective intelligence of humans making it and we can get into. Obviously people voting, yes, no on a binary proposition, where both sides, both versions of the proposition suck. If it goes through it, benefits something and harm something else. If it doesn't go through, the thing it would benefit is now harmed, because the proposition was just designed stupidly to begin with, it didn't factor how interconnected everything was, so the yes no on it can't not polarize the population, like that's just a stupid system of collective Intelligence right like that's just, we can just do much much better. Where, before you make a proposition, you actually do the sense making of what are all the interconnected things. What are all the values you take, those as design constraints to go through a better proposition, crafting process of what is the best synergistic satisfier with the least theory of trade-offs possible and what are better voting systems than binary that inherently polarize the population. So i think we can do a radically better job of systems of collective intelligence and a radically better job of education, of people to be able to participate. With these things and an incentive system where, as people become more educated in topics, they actually get more ability to influence those topics um, we could get into that. It'S beyond the scope of this initial introduction, but one of the issues is that, where there's more information that anybody can process right, the information singularity, there's more journal articles on a topic than any expert can read. So nobody can ever be an expert on anything. How do we solve that? One idea? Is we don't and we simply need ai's to run the world? The other idea is, we have to be able to merge with the ais through some kind of brain computer interface, or something like that, i would say. Another version is that when you look at even the current state of generator ai, which is really not advanced, compared to what will be a year from now or five years from now, because it's advancing so rapidly. But you look at the current state, which would be say gpt-3 today, open ai. It can use exclusively natural language input. I don't have to be able to program, i just speak to it and it understands my words and it does stuff based on understanding the words. That'S amazing right like go watch the dolly and the gpt3 um demos on youtube to just get a sense of what they can currently do, because it's mind-blowing - and this only thing more mind-blowing - is the speed at which it's advancing the destructive capabilities of this. Don'T take thinking very hard to imagine and they're very near-term. The beneficial cases, but that are narrow, are also really obvious. The omni-beneficial facilitation. How do we create better collective intelligence and governance? Writ large is not as obvious until you really start to think about it, but then it is amazing. I i feel - and this is not a techno optimist - answer - i'm very acutely aware that the technology on the current trajectory is most likely catastrophic and what it takes to make. The other version of it is value systems that have to be able to bind guide and direct it and influence social systems that have that create the capacity to bind guide and direct markets and technology in a way they don't currently have. But it is saying that the technology is new capacity and that that capacity, if rightly directed, does make new things possible in the same way that the printing press made democracy possible where it wouldn't have been before, because without any newspaper and without textbooks, you can't have A comprehensively educated and informed society when hand copying a book is so hard and expensive. Only a nobility class with the wealth can have access to it, of course, that technology of the printing press did make possible different types of coordination than was possible before obviously the internet. Anybody talked to anybody anywhere in the world made possible different types of coordination. Make possible and orient it to happen that way or different um, while most people could use their phone to access any information in the world what they end up doing on. It is usually not that interesting examples because of the nature of the choice architectures that they're engaging with in their user interfaces and the choice architectures, are that the user interfaces that they engage with have goals for them that are other than that person's highest goals for Themselves and they're effective um, so this is not a. I will save the world, it's also not a. I will necessarily destroy the world. This is a since we have the power to create such powerful technology. We need the orientation to ha to think about what wise application of that technology is, and the intersection of comparable wisdom to guide right design and development of that technology. That, in turn, then, is developing wisdom and everyone else based on their interface, in the same way that facebook can increase, and it's a known thing, you can change the algorithm that is affecting what's in people's newsfeed and they get more depressed or suicidal or more body Image issues or not or because we're affected by what we untake could we create information platforms that, rather than doubling down on my existing bias, were oriented to help expose me to information that would that is antithetical to the things that i currently think, but in the Most compelling cases so as to increase my kind of dialectical, awareness that, rather than orient me to more people that are like the people. I already know that orient me to people that are most unlike the people. I already know to increase the total connectivity of my network that, rather than maximizing for things like my engagement and time on site, it was maximizing for things like um as i was showing the capacity to take and synthesize more perspectives. The content that did that got upregulated in the algorithm right are there ways that that same type of technology could be applied. That would be wisdom, advancing. Of course. This sounds scary because, like who's idea of wisdom and who's going to control that, but it's important to get it's already doing it right, it's already controlling minds at scale. So it's not a question of do we do it or not it's. How do we do it? Since it is happening right and now, the core question of well, what how do we adjudicate, what right use of the technology when you realize that the technology is not only radically affecting the earth physically, but radically affecting our mind society's cultures? Now, this again, the the wisdom of gods to deal with the power of them become central. What is the right use of that? What are wrong uses? Where should that be bound? How do we understand this? How do we make sure that the binding, a particular application of a problem, doesn't make another worse problem? So we don't like a particular kind of partisan-led censorship, so we want more free speech. But if a particular approach to free speech also means a lot more um ubiquitous deep fakes and an upregulation of the worst voices, because they get most uh tension that creates eliciting of violence and the breakdowns of democracy, it's like okay, the obvious answers are all wrong. Right because the problem space is more interconnected and more complex, so we have to hold all those problem, spaces together and think about it. But we come back to this. You know gpt3, like ai type tech, solving the info singularity. So when you put something to gpthree, it's not going to find a existing web page for you the way that a search engine would it can generate bespoke content right, you can say, write me an article in the voice of such and such it factors these kinds Of facts and orients and towards this kind of conclusion and whatever, and it can do that and progressively more and more effectively more and more turing tests passing across more domains. Well, what if that just becomes the future of search right, where, if i'm searching for the information that could inform a particular choice, we're wanting to make we're wanting to do something regarding grid security but grid security? I need to know all the things about what really affects good security and how what other environmental and national security - and you know, et cetera issues - are connected to that right. Now i can get billions of search results. I can't read billions of search results. That'S not useful for me. If i uh can the ai read billions of search results, find the information that is decision informing based on parameters that i'm specifying and create new bespoke content? That is the synthesis of that for incision decision and forming not decision creating the groups of people doing collective intelligence are still making the decisions, but now with the ability to take a synthesized or refined set of more information than they can process, it is pre-processed into Decision-Making information now you say well who controls that algorithm, because that's a big deal well, what? If you can do it lots of different ways right? What if you can um put different criteria for how to inform it and in the collective intelligence system? All the agents have the capacity to do that kind of thing. There are heaps of challenges and problems that we have to solve here, but this is an example of ways that coordination technology, the multi-polar trap, is a coordination failure. It'S a collective action failure. The dunbar number can be thought of as an upper boundary of a particular type of coordination capacity where everybody can know everybody and see what everybody's doing talk to everybody, and so the collective activity is able to be bound through very high bandwidth communication. Once we start getting larger than that early empires, we got command and control hierarchies and we started to get all the problems that we see in the world today that are um now at the scale that we are driving catastrophic risks, um democracies, where how do we Have a much larger system, or rather than have uh somebody at the top rule, whether it's one monarch or some small kind of uh, nobility class or whatever. How do we at least have since we can't get everybody at scale to agree? The way that we could? Maybe get everybody at a tiny scale to agree, but just a minority agreeing seems like a bummer. Can we at least get a majority to agree, but then, of course, that thing intrinsically ends up driving polarization in its own decay, and so can we make the types of transparency and the types of incentive alignment the types of capacity for deterrence that would exist at A small tribal type scale possible at much much larger scales, where the increase in effective coordination starts to be uh, more effective right. You start to reduce a huge amount of waste and duplication, and you know um failures of those kinds, and so what makes this thing adaptive and selective in a kind of game theoretic situation is also what makes it healthy in terms of the health of the people Inside in terms of classes, relative people relative to each other and its relationship with the environment, i think that something like that has to be the answer, and i think that it's interesting that the technologies that have the most destructive capability, i think, also have the most And uniquely facilitative capability for um solving collective action or facilitating us solving collective action problems.
